
train images/masks shape:
(18, 1, 565, 565)
train images range (min-max): 0.0 - 1.0
train masks are within 0-1

positive patches per full image: 5000
negative patches per full image: 5000

train PATCHES images/masks shape:
(180000, 1, 27, 27)
train PATCHES images range (min-max): 0.0 - 1.0

train images/masks shape:
(2, 1, 565, 565)
train images range (min-max): 0.0 - 1.0
train masks are within 0-1

patches per full image: 9000

train PATCHES images/masks shape:
(18000, 1, 27, 27)
train PATCHES images range (min-max): 0.0 - 1.0
Check: final output of the network:
(None, 2)
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 1, 27, 27)         0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 32, 27, 27)        320       
_________________________________________________________________
dropout_1 (Dropout)          (None, 32, 27, 27)        0         
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 32, 13, 13)        0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 5408)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 2)                 10818     
_________________________________________________________________
activation_1 (Activation)    (None, 2)                 0         
=================================================================
Total params: 11,138
Trainable params: 11,138
Non-trainable params: 0
_________________________________________________________________



1  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from inf to 0.42553, saving model to ./log_normalisation/cnn/log_normalisation-weights-0.42553.h5
Epoch 00000: val_loss improved from inf to 0.42553, saving model to ./log_normalisation/cnn/log_normalisation_best_weights.h5
43s - loss: 0.5621 - acc: 0.6818 - val_loss: 0.4255 - val_acc: 0.9108
(180000,) (180000,)
83131 6869
31195 58805

FA FR TA TR 0.0763222222222 0.346611111111 0.653388888889 0.923677777778

VALIDATION DATA
0.910833333333 0.425534420755
(18000,) (18000,)
15409 990
615 986

FA FR TA TR 0.0603695347277 0.384134915678 0.615865084322 0.939630465272
0.425534420755  - val loss
999999  - final_loss
Validation Loss decreased. Great work



2  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
36s - loss: 0.4626 - acc: 0.7909 - val_loss: 0.4278 - val_acc: 0.8709
(180000,) (180000,)
77464 12536
18956 71044

FA FR TA TR 0.139288888889 0.210622222222 0.789377777778 0.860711111111

VALIDATION DATA
0.870944444444 0.42779175255
(18000,) (18000,)
14416 1983
340 1261

FA FR TA TR 0.120922007439 0.212367270456 0.787632729544 0.879077992561
0.42779175255  - val loss
0.425534420755  - final_loss
Inside Plateau 1



2  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from 0.42553 to 0.29285, saving model to ./log_normalisation/cnn/log_normalisation-weights-0.29285.h5
Epoch 00000: val_loss improved from 0.42553 to 0.29285, saving model to ./log_normalisation/cnn/log_normalisation_best_weights.h5
38s - loss: 0.4267 - acc: 0.8123 - val_loss: 0.2929 - val_acc: 0.9351
(180000,) (180000,)
85829 4171
33511 56489

FA FR TA TR 0.0463444444444 0.372344444444 0.627655555556 0.953655555556

VALIDATION DATA
0.935055555556 0.292853257947
(18000,) (18000,)
15924 475
694 907

FA FR TA TR 0.0289651808037 0.433479075578 0.566520924422 0.971034819196
0.292853257947  - val loss
0.425534420755  - final_loss
Validation Loss decreased. Great work



3  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
38s - loss: 0.4042 - acc: 0.8256 - val_loss: 0.3839 - val_acc: 0.8853
(180000,) (180000,)
80373 9627
17731 72269

FA FR TA TR 0.106966666667 0.197011111111 0.802988888889 0.893033333333

VALIDATION DATA
0.885277777778 0.383893987232
(18000,) (18000,)
14600 1799
266 1335

FA FR TA TR 0.109701811086 0.166146158651 0.833853841349 0.890298188914
0.383893987232  - val loss
0.292853257947  - final_loss
Inside Plateau 1



3  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
37s - loss: 0.3834 - acc: 0.8358 - val_loss: 0.5211 - val_acc: 0.7212
(180000,) (180000,)
64209 25791
5332 84668

FA FR TA TR 0.286566666667 0.0592444444444 0.940755555556 0.713433333333

VALIDATION DATA
0.721166666667 0.521121916029
(18000,) (18000,)
11448 4951
68 1533

FA FR TA TR 0.301908652967 0.0424734540912 0.957526545909 0.698091347033
0.521121916029  - val loss
0.292853257947  - final_loss
Inside Plateau 2



3  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from 0.29285 to 0.23129, saving model to ./log_normalisation/cnn/log_normalisation-weights-0.23129.h5
Epoch 00000: val_loss improved from 0.29285 to 0.23129, saving model to ./log_normalisation/cnn/log_normalisation_best_weights.h5
37s - loss: 0.3672 - acc: 0.8450 - val_loss: 0.2313 - val_acc: 0.9445
(180000,) (180000,)
87237 2763
33005 56995

FA FR TA TR 0.0307 0.366722222222 0.633277777778 0.9693

VALIDATION DATA
0.9445 0.23129458287
(18000,) (18000,)
16069 330
669 932

FA FR TA TR 0.0201231782426 0.417863835103 0.582136164897 0.979876821757
0.23129458287  - val loss
0.292853257947  - final_loss
Validation Loss decreased. Great work



4  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
38s - loss: 0.3510 - acc: 0.8533 - val_loss: 0.3195 - val_acc: 0.9228
(180000,) (180000,)
82792 7208
17163 72837

FA FR TA TR 0.0800888888889 0.1907 0.8093 0.919911111111

VALIDATION DATA
0.922777777778 0.319487630844
(18000,) (18000,)
15315 1084
306 1295

FA FR TA TR 0.0661015915605 0.19113054341 0.80886945659 0.93389840844
0.319487630844  - val loss
0.23129458287  - final_loss
Inside Plateau 1



4  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
40s - loss: 0.3412 - acc: 0.8593 - val_loss: 0.3201 - val_acc: 0.9163
(180000,) (180000,)
82951 7049
15875 74125

FA FR TA TR 0.0783222222222 0.176388888889 0.823611111111 0.921677777778

VALIDATION DATA
0.916333333333 0.320059680303
(18000,) (18000,)
15128 1271
235 1366

FA FR TA TR 0.0775047258979 0.146783260462 0.853216739538 0.922495274102
0.320059680303  - val loss
0.23129458287  - final_loss
Inside Plateau 2



4  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
39s - loss: 0.3311 - acc: 0.8646 - val_loss: 0.6490 - val_acc: 0.5729
(180000,) (180000,)
47453 42547
1638 88362

FA FR TA TR 0.472744444444 0.0182 0.9818 0.527255555556

VALIDATION DATA
0.572888888889 0.64895202891
(18000,) (18000,)
8730 7669
19 1582

FA FR TA TR 0.467650466492 0.0118675827608 0.988132417239 0.532349533508
0.64895202891  - val loss
0.23129458287  - final_loss
Inside Plateau 3



4  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from 0.23129 to 0.21031, saving model to ./log_normalisation/cnn/log_normalisation-weights-0.21031.h5
Epoch 00000: val_loss improved from 0.23129 to 0.21031, saving model to ./log_normalisation/cnn/log_normalisation_best_weights.h5
37s - loss: 0.3235 - acc: 0.8687 - val_loss: 0.2103 - val_acc: 0.9442
(180000,) (180000,)
86643 3357
24983 65017

FA FR TA TR 0.0373 0.277588888889 0.722411111111 0.9627

VALIDATION DATA
0.944222222222 0.2103093833
(18000,) (18000,)
15835 564
440 1161

FA FR TA TR 0.0343923409964 0.274828232355 0.725171767645 0.965607659004
0.2103093833  - val loss
0.23129458287  - final_loss
Validation Loss decreased. Great work



5  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
38s - loss: 0.3183 - acc: 0.8716 - val_loss: 0.2477 - val_acc: 0.9374
(180000,) (180000,)
84540 5460
17310 72690

FA FR TA TR 0.0606666666667 0.192333333333 0.807666666667 0.939333333333

VALIDATION DATA
0.937388888889 0.247700848023
(18000,) (18000,)
15573 826
301 1300

FA FR TA TR 0.0503689249344 0.188007495315 0.811992504685 0.949631075066
0.247700848023  - val loss
0.2103093833  - final_loss
Inside Plateau 1



5  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
37s - loss: 0.3133 - acc: 0.8738 - val_loss: 0.3822 - val_acc: 0.8746
(180000,) (180000,)
77310 12690
7567 82433

FA FR TA TR 0.141 0.0840777777778 0.915922222222 0.859

VALIDATION DATA
0.874555555556 0.382195382807
(18000,) (18000,)
14240 2159
99 1502

FA FR TA TR 0.131654369169 0.0618363522798 0.93816364772 0.868345630831
0.382195382807  - val loss
0.2103093833  - final_loss
Inside Plateau 2



5  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from 0.21031 to 0.20809, saving model to ./log_normalisation/cnn/log_normalisation-weights-0.20809.h5
Epoch 00000: val_loss improved from 0.21031 to 0.20809, saving model to ./log_normalisation/cnn/log_normalisation_best_weights.h5
37s - loss: 0.3098 - acc: 0.8754 - val_loss: 0.2081 - val_acc: 0.9451
(180000,) (180000,)
86507 3493
23044 66956

FA FR TA TR 0.0388111111111 0.256044444444 0.743955555556 0.961188888889

VALIDATION DATA
0.945111111111 0.208085446186
(18000,) (18000,)
15778 621
367 1234

FA FR TA TR 0.0378681626928 0.229231730169 0.770768269831 0.962131837307
0.208085446186  - val loss
0.2103093833  - final_loss
Validation Loss decreased. Great work



6  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
38s - loss: 0.3072 - acc: 0.8780 - val_loss: 0.2369 - val_acc: 0.9393
(180000,) (180000,)
84775 5225
16899 73101

FA FR TA TR 0.0580555555556 0.187766666667 0.812233333333 0.941944444444

VALIDATION DATA
0.939333333333 0.236926090638
(18000,) (18000,)
15599 800
292 1309

FA FR TA TR 0.0487834624062 0.182386008745 0.817613991255 0.951216537594
0.236926090638  - val loss
0.208085446186  - final_loss
Inside Plateau 1



6  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
39s - loss: 0.3047 - acc: 0.8795 - val_loss: 0.3231 - val_acc: 0.9103
(180000,) (180000,)
80471 9529
9762 80238

FA FR TA TR 0.105877777778 0.108466666667 0.891533333333 0.894122222222

VALIDATION DATA
0.910333333333 0.323121221542
(18000,) (18000,)
14923 1476
138 1463

FA FR TA TR 0.0900054881395 0.0861961274204 0.91380387258 0.90999451186
0.323121221542  - val loss
0.208085446186  - final_loss
Inside Plateau 2



6  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
38s - loss: 0.3028 - acc: 0.8803 - val_loss: 0.2134 - val_acc: 0.9436
(180000,) (180000,)
85855 4145
20253 69747

FA FR TA TR 0.0460555555556 0.225033333333 0.774966666667 0.953944444444

VALIDATION DATA
0.943611111111 0.213418247739
(18000,) (18000,)
15730 669
346 1255

FA FR TA TR 0.0407951704372 0.21611492817 0.78388507183 0.959204829563
0.213418247739  - val loss
0.208085446186  - final_loss
Inside Plateau 3



6  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
38s - loss: 0.3008 - acc: 0.8820 - val_loss: 0.2383 - val_acc: 0.9372
(180000,) (180000,)
84741 5259
15721 74279

FA FR TA TR 0.0584333333333 0.174677777778 0.825322222222 0.941566666667

VALIDATION DATA
0.937222222222 0.238269014213
(18000,) (18000,)
15511 888
242 1359

FA FR TA TR 0.0541496432709 0.151155527795 0.848844472205 0.945850356729
0.238269014213  - val loss
0.208085446186  - final_loss
Reducing the learning rate by half



6  iteration
0.005  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
39s - loss: 0.2909 - acc: 0.8878 - val_loss: 0.2876 - val_acc: 0.9234
(180000,) (180000,)
82821 7179
12085 77915

FA FR TA TR 0.0797666666667 0.134277777778 0.865722222222 0.920233333333

VALIDATION DATA
0.923444444444 0.287610487395
(18000,) (18000,)
15188 1211
167 1434

FA FR TA TR 0.0738459662175 0.104309806371 0.895690193629 0.926154033783
0.287610487395  - val loss
0.208085446186  - final_loss
Inside Plateau 1



6  iteration
0.005  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
38s - loss: 0.2901 - acc: 0.8879 - val_loss: 0.3134 - val_acc: 0.9119
(180000,) (180000,)
81639 8361
10575 79425

FA FR TA TR 0.0929 0.1175 0.8825 0.9071

VALIDATION DATA
0.911888888889 0.31340578964
(18000,) (18000,)
14949 1450
136 1465

FA FR TA TR 0.0884200256113 0.0849469081824 0.915053091818 0.911579974389
0.31340578964  - val loss
0.208085446186  - final_loss
Inside Plateau 2



6  iteration
0.005  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
40s - loss: 0.2893 - acc: 0.8883 - val_loss: 0.2208 - val_acc: 0.9407
(180000,) (180000,)
85117 4883
17484 72516

FA FR TA TR 0.0542555555556 0.194266666667 0.805733333333 0.945744444444

VALIDATION DATA
0.940666666667 0.220797415588
(18000,) (18000,)
15625 774
294 1307

FA FR TA TR 0.047197999878 0.183635227983 0.816364772017 0.952802000122
0.220797415588  - val loss
0.208085446186  - final_loss
Inside Plateau 3



6  iteration
0.005  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
38s - loss: 0.2885 - acc: 0.8882 - val_loss: 0.2609 - val_acc: 0.9309
(180000,) (180000,)
83719 6281
13504 76496

FA FR TA TR 0.0697888888889 0.150044444444 0.849955555556 0.930211111111

VALIDATION DATA
0.930888888889 0.260933523311
(18000,) (18000,)
15357 1042
202 1399

FA FR TA TR 0.0635404597841 0.126171143036 0.873828856964 0.936459540216
0.260933523311  - val loss
0.208085446186  - final_loss
Reducing the learning rate by half



6  iteration
0.0025  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
38s - loss: 0.2845 - acc: 0.8908 - val_loss: 0.3611 - val_acc: 0.8871
(180000,) (180000,)
78189 11811
7349 82651

FA FR TA TR 0.131233333333 0.0816555555556 0.918344444444 0.868766666667

VALIDATION DATA
0.887055555556 0.361093969027
(18000,) (18000,)
14452 1947
86 1515

FA FR TA TR 0.118726751631 0.053716427233 0.946283572767 0.881273248369
0.361093969027  - val loss
0.208085446186  - final_loss
Inside Plateau 1



6  iteration
0.0025  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
38s - loss: 0.2845 - acc: 0.8909 - val_loss: 0.2829 - val_acc: 0.9232
(180000,) (180000,)
82640 7360
11615 78385

FA FR TA TR 0.0817777777778 0.129055555556 0.870944444444 0.918222222222

VALIDATION DATA
0.923166666667 0.282900648064
(18000,) (18000,)
15181 1218
165 1436

FA FR TA TR 0.0742728215135 0.103060587133 0.896939412867 0.925727178486
0.282900648064  - val loss
0.208085446186  - final_loss
Inside Plateau 2



6  iteration
0.0025  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
39s - loss: 0.2839 - acc: 0.8909 - val_loss: 0.3251 - val_acc: 0.9056
(180000,) (180000,)
79722 10278
8523 81477

FA FR TA TR 0.1142 0.0947 0.9053 0.8858

VALIDATION DATA
0.905611111111 0.325143265698
(18000,) (18000,)
14814 1585
114 1487

FA FR TA TR 0.0966522348924 0.0712054965646 0.928794503435 0.903347765108
0.325143265698  - val loss
0.208085446186  - final_loss
Inside Plateau 3



6  iteration
0.0025  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
39s - loss: 0.2836 - acc: 0.8910 - val_loss: 0.2770 - val_acc: 0.9282
(180000,) (180000,)
82763 7237
11715 78285

FA FR TA TR 0.0804111111111 0.130166666667 0.869833333333 0.919588888889

VALIDATION DATA
0.928166666667 0.276990400619
(18000,) (18000,)
15276 1123
170 1431

FA FR TA TR 0.0684797853528 0.106183635228 0.893816364772 0.931520214647
0.276990400619  - val loss
0.208085446186  - final_loss
Reducing the learning rate by half



6  iteration
0.00125  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
41s - loss: 0.2822 - acc: 0.8924 - val_loss: 0.2826 - val_acc: 0.9235
(180000,) (180000,)
82485 7515
11386 78614

FA FR TA TR 0.0835 0.126511111111 0.873488888889 0.9165

VALIDATION DATA
0.9235 0.282566742526
(18000,) (18000,)
15180 1219
158 1443

FA FR TA TR 0.0743338008415 0.0986883198001 0.9013116802 0.925666199158
0.282566742526  - val loss
0.208085446186  - final_loss
Inside Plateau 1



6  iteration
0.00125  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
38s - loss: 0.2822 - acc: 0.8919 - val_loss: 0.2885 - val_acc: 0.9224
(180000,) (180000,)
82294 7706
11142 78858

FA FR TA TR 0.0856222222222 0.1238 0.8762 0.914377777778

VALIDATION DATA
0.922444444444 0.288456760777
(18000,) (18000,)
15150 1249
147 1454

FA FR TA TR 0.0761631806817 0.0918176139913 0.908182386009 0.923836819318
0.288456760777  - val loss
0.208085446186  - final_loss
Inside Plateau 2



6  iteration
0.00125  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
38s - loss: 0.2816 - acc: 0.8920 - val_loss: 0.2707 - val_acc: 0.9272
(180000,) (180000,)
82993 7007
12109 77891

FA FR TA TR 0.0778555555556 0.134544444444 0.865455555556 0.922144444444

VALIDATION DATA
0.927222222222 0.270725886371
(18000,) (18000,)
15264 1135
175 1426

FA FR TA TR 0.0692115372889 0.109306683323 0.890693316677 0.930788462711
0.270725886371  - val loss
0.208085446186  - final_loss
Inside Plateau 3



6  iteration
0.00125  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
38s - loss: 0.2816 - acc: 0.8920 - val_loss: 0.3116 - val_acc: 0.9129
(180000,) (180000,)
80749 9251
9361 80639

FA FR TA TR 0.102788888889 0.104011111111 0.895988888889 0.897211111111

VALIDATION DATA
0.912944444444 0.311641993602
(18000,) (18000,)
14952 1447
120 1481

FA FR TA TR 0.0882370876273 0.0749531542786 0.925046845721 0.911762912373
0.311641993602  - val loss
0.208085446186  - final_loss
Reducing the learning rate by half
   32/18000 [..............................] - ETA: 1s  608/18000 [>.............................] - ETA: 1s 1376/18000 [=>............................] - ETA: 1s 2176/18000 [==>...........................] - ETA: 1s 3008/18000 [====>.........................] - ETA: 1s 3840/18000 [=====>........................] - ETA: 0s 4608/18000 [======>.......................] - ETA: 0s 5408/18000 [========>.....................] - ETA: 0s 6176/18000 [=========>....................] - ETA: 0s 6944/18000 [==========>...................] - ETA: 0s 7712/18000 [===========>..................] - ETA: 0s 8480/18000 [=============>................] - ETA: 0s 9280/18000 [==============>...............] - ETA: 0s10080/18000 [===============>..............] - ETA: 0s10880/18000 [=================>............] - ETA: 0s11712/18000 [==================>...........] - ETA: 0s12480/18000 [===================>..........] - ETA: 0s13312/18000 [=====================>........] - ETA: 0s14144/18000 [======================>.......] - ETA: 0s14752/18000 [=======================>......] - ETA: 0s15264/18000 [========================>.....] - ETA: 0s16096/18000 [=========================>....] - ETA: 0s16960/18000 [===========================>..] - ETA: 0s17824/18000 [============================>.] - ETA: 0s
ROC AREA:  0.961812752785
(18000,) (18000,)

train images/masks shape:
(18, 1, 565, 565)
train images range (min-max): 0.0 - 1.0
train masks are within 0-1

positive patches per full image: 5000
negative patches per full image: 5000

train PATCHES images/masks shape:
(180000, 1, 27, 27)
train PATCHES images range (min-max): 0.0 - 1.0

train images/masks shape:
(2, 1, 565, 565)
train images range (min-max): 0.0 - 1.0
train masks are within 0-1

patches per full image: 9000

train PATCHES images/masks shape:
(18000, 1, 27, 27)
train PATCHES images range (min-max): 0.0 - 1.0
Check: final output of the network:
(None, 2)
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 1, 27, 27)         0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 32, 27, 27)        320       
_________________________________________________________________
dropout_1 (Dropout)          (None, 32, 27, 27)        0         
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 32, 13, 13)        0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 5408)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 2)                 10818     
_________________________________________________________________
activation_1 (Activation)    (None, 2)                 0         
=================================================================
Total params: 11,138
Trainable params: 11,138
Non-trainable params: 0
_________________________________________________________________



1  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1

train images/masks shape:
(18, 1, 565, 565)
train images range (min-max): 0.0 - 1.0
train masks are within 0-1

positive patches per full image: 5000
negative patches per full image: 5000

train PATCHES images/masks shape:
(180000, 1, 27, 27)
train PATCHES images range (min-max): 0.0 - 1.0

train images/masks shape:
(2, 1, 565, 565)
train images range (min-max): 0.0 - 1.0
train masks are within 0-1

patches per full image: 9000

train PATCHES images/masks shape:
(18000, 1, 27, 27)
train PATCHES images range (min-max): 0.0 - 1.0
Check: final output of the network:
(None, 2)
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 1, 27, 27)         0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 32, 27, 27)        320       
_________________________________________________________________
dropout_1 (Dropout)          (None, 32, 27, 27)        0         
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 32, 13, 13)        0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 5408)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 2)                 10818     
_________________________________________________________________
activation_1 (Activation)    (None, 2)                 0         
=================================================================
Total params: 11,138
Trainable params: 11,138
Non-trainable params: 0
_________________________________________________________________



1  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1

train images/masks shape:
(18, 1, 565, 565)
train images range (min-max): 0.0 - 1.0
train masks are within 0-1

positive patches per full image: 5000
negative patches per full image: 5000

train PATCHES images/masks shape:
(180000, 1, 27, 27)
train PATCHES images range (min-max): 0.0 - 1.0

train images/masks shape:
(2, 1, 565, 565)
train images range (min-max): 0.0 - 1.0
train masks are within 0-1

patches per full image: 9000

train PATCHES images/masks shape:
(18000, 1, 27, 27)
train PATCHES images range (min-max): 0.0 - 1.0
Check: final output of the network:
(None, 2)
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 1, 27, 27)         0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 32, 27, 27)        320       
_________________________________________________________________
dropout_1 (Dropout)          (None, 32, 27, 27)        0         
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 32, 13, 13)        0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 5408)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 2)                 10818     
_________________________________________________________________
activation_1 (Activation)    (None, 2)                 0         
=================================================================
Total params: 11,138
Trainable params: 11,138
Non-trainable params: 0
_________________________________________________________________



1  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from inf to 0.42559, saving model to ./log/log_normalisation/cnn/log_normalisation-weights-0.42559.h5
Epoch 00000: val_loss improved from inf to 0.42559, saving model to ./log/log_normalisation/cnn/log_normalisation_best_weights.h5
42s - loss: 0.5621 - acc: 0.6819 - val_loss: 0.4256 - val_acc: 0.9108

train images/masks shape:
(18, 1, 565, 565)
train images range (min-max): 0.0 - 1.0
train masks are within 0-1

positive patches per full image: 5000
negative patches per full image: 5000

train PATCHES images/masks shape:
(180000, 1, 27, 27)
train PATCHES images range (min-max): 0.0 - 1.0

train images/masks shape:
(2, 1, 565, 565)
train images range (min-max): 0.0 - 1.0
train masks are within 0-1

patches per full image: 9000

train PATCHES images/masks shape:
(18000, 1, 27, 27)
train PATCHES images range (min-max): 0.0 - 1.0
Check: final output of the network:
(None, 2)
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 1, 27, 27)         0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 32, 27, 27)        320       
_________________________________________________________________
dropout_1 (Dropout)          (None, 32, 27, 27)        0         
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 32, 13, 13)        0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 5408)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 2)                 10818     
_________________________________________________________________
activation_1 (Activation)    (None, 2)                 0         
=================================================================
Total params: 11,138
Trainable params: 11,138
Non-trainable params: 0
_________________________________________________________________



1  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from inf to 0.42558, saving model to ./log/log_normalisation/cnn/log_normalisation-weights-0.42558.h5
Epoch 00000: val_loss improved from inf to 0.42558, saving model to ./log/log_normalisation/cnn/log_normalisation_best_weights.h5
37s - loss: 0.5621 - acc: 0.6818 - val_loss: 0.4256 - val_acc: 0.9108
(180000,) (180000,)
83117 6883
31173 58827

FA FR TA TR 0.0764777777778 0.346366666667 0.653633333333 0.923522222222

VALIDATION DATA
0.910833333333 0.425576570696
(18000,) (18000,)
15409 990
615 986

FA FR TA TR 0.0603695347277 0.384134915678 0.615865084322 0.939630465272
0.425576570696  - val loss
999999  - final_loss
Validation Loss decreased. Great work



2  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
36s - loss: 0.4626 - acc: 0.7910 - val_loss: 0.4280 - val_acc: 0.8708
(180000,) (180000,)
77434 12566
18925 71075

FA FR TA TR 0.139622222222 0.210277777778 0.789722222222 0.860377777778

VALIDATION DATA
0.870833333333 0.428044902696
(18000,) (18000,)
14413 1986
339 1262

FA FR TA TR 0.121104945424 0.211742660837 0.788257339163 0.878895054576
0.428044902696  - val loss
0.425576570696  - final_loss
Inside Plateau 1



2  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from 0.42558 to 0.29398, saving model to ./log/log_normalisation/cnn/log_normalisation-weights-0.29398.h5
Epoch 00000: val_loss improved from 0.42558 to 0.29398, saving model to ./log/log_normalisation/cnn/log_normalisation_best_weights.h5
36s - loss: 0.4268 - acc: 0.8123 - val_loss: 0.2940 - val_acc: 0.9348
(180000,) (180000,)
85802 4198
33360 56640

FA FR TA TR 0.0466444444444 0.370666666667 0.629333333333 0.953355555556

VALIDATION DATA
0.934833333333 0.29398045071
(18000,) (18000,)
15917 482
691 910

FA FR TA TR 0.0293920360998 0.431605246721 0.568394753279 0.9706079639
0.29398045071  - val loss
0.425576570696  - final_loss
Validation Loss decreased. Great work



3  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
36s - loss: 0.4041 - acc: 0.8258 - val_loss: 0.3833 - val_acc: 0.8859
(180000,) (180000,)
80440 9560
17834 72166

FA FR TA TR 0.106222222222 0.198155555556 0.801844444444 0.893777777778

VALIDATION DATA
0.885944444444 0.383302631696
(18000,) (18000,)
14613 1786
267 1334

FA FR TA TR 0.108909079822 0.16677076827 0.83322923173 0.891090920178
0.383302631696  - val loss
0.29398045071  - final_loss
Inside Plateau 1



3  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
36s - loss: 0.3834 - acc: 0.8360 - val_loss: 0.5197 - val_acc: 0.7237
(180000,) (180000,)
64269 25731
5310 84690

FA FR TA TR 0.2859 0.059 0.941 0.7141

VALIDATION DATA
0.723666666667 0.519677913189
(18000,) (18000,)
11492 4907
67 1534

FA FR TA TR 0.299225562534 0.0418488444722 0.958151155528 0.700774437466
0.519677913189  - val loss
0.29398045071  - final_loss
Inside Plateau 2



3  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from 0.29398 to 0.23145, saving model to ./log/log_normalisation/cnn/log_normalisation-weights-0.23145.h5
Epoch 00000: val_loss improved from 0.29398 to 0.23145, saving model to ./log/log_normalisation/cnn/log_normalisation_best_weights.h5
37s - loss: 0.3672 - acc: 0.8450 - val_loss: 0.2315 - val_acc: 0.9446
(180000,) (180000,)
87248 2752
33040 56960

FA FR TA TR 0.0305777777778 0.367111111111 0.632888888889 0.969422222222

VALIDATION DATA
0.944611111111 0.231450217432
(18000,) (18000,)
16070 329
668 933

FA FR TA TR 0.0200621989146 0.417239225484 0.582760774516 0.979937801085
0.231450217432  - val loss
0.29398045071  - final_loss
Validation Loss decreased. Great work



4  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
36s - loss: 0.3510 - acc: 0.8533 - val_loss: 0.3194 - val_acc: 0.9228
(180000,) (180000,)
82807 7193
17201 72799

FA FR TA TR 0.0799222222222 0.191122222222 0.808877777778 0.920077777778

VALIDATION DATA
0.922777777778 0.319369767692
(18000,) (18000,)
15315 1084
306 1295

FA FR TA TR 0.0661015915605 0.19113054341 0.80886945659 0.93389840844
0.319369767692  - val loss
0.231450217432  - final_loss
Inside Plateau 1



4  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
37s - loss: 0.3412 - acc: 0.8592 - val_loss: 0.3195 - val_acc: 0.9166
(180000,) (180000,)
82982 7018
15924 74076

FA FR TA TR 0.0779777777778 0.176933333333 0.823066666667 0.922022222222

VALIDATION DATA
0.916555555556 0.319450711303
(18000,) (18000,)
15132 1267
235 1366

FA FR TA TR 0.0772608085859 0.146783260462 0.853216739538 0.922739191414
0.319450711303  - val loss
0.231450217432  - final_loss
Inside Plateau 2



4  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
37s - loss: 0.3310 - acc: 0.8645 - val_loss: 0.6474 - val_acc: 0.5749
(180000,) (180000,)
47556 42444
1646 88354

FA FR TA TR 0.4716 0.0182888888889 0.981711111111 0.5284

VALIDATION DATA
0.574888888889 0.647394277467
(18000,) (18000,)
8766 7633
19 1582

FA FR TA TR 0.465455210684 0.0118675827608 0.988132417239 0.534544789316
0.647394277467  - val loss
0.231450217432  - final_loss
Inside Plateau 3



4  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from 0.23145 to 0.21068, saving model to ./log/log_normalisation/cnn/log_normalisation-weights-0.21068.h5
Epoch 00000: val_loss improved from 0.23145 to 0.21068, saving model to ./log/log_normalisation/cnn/log_normalisation_best_weights.h5
37s - loss: 0.3235 - acc: 0.8688 - val_loss: 0.2107 - val_acc: 0.9443
(180000,) (180000,)
86632 3368
24942 65058

FA FR TA TR 0.0374222222222 0.277133333333 0.722866666667 0.962577777778

VALIDATION DATA
0.944277777778 0.210683836354
(18000,) (18000,)
15833 566
437 1164

FA FR TA TR 0.0345142996524 0.272954403498 0.727045596502 0.965485700348
0.210683836354  - val loss
0.231450217432  - final_loss
Validation Loss decreased. Great work



5  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
37s - loss: 0.3183 - acc: 0.8717 - val_loss: 0.2480 - val_acc: 0.9374
(180000,) (180000,)
84536 5464
17287 72713

FA FR TA TR 0.0607111111111 0.192077777778 0.807922222222 0.939288888889

VALIDATION DATA
0.937444444444 0.247982615312
(18000,) (18000,)
15574 825
301 1300

FA FR TA TR 0.0503079456064 0.188007495315 0.811992504685 0.949692054394
0.247982615312  - val loss
0.210683836354  - final_loss
Inside Plateau 1



5  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
37s - loss: 0.3133 - acc: 0.8738 - val_loss: 0.3840 - val_acc: 0.8729
(180000,) (180000,)
77178 12822
7500 82500

FA FR TA TR 0.142466666667 0.0833333333333 0.916666666667 0.857533333333

VALIDATION DATA
0.872944444444 0.383991172923
(18000,) (18000,)
14210 2189
98 1503

FA FR TA TR 0.133483749009 0.0612117426608 0.938788257339 0.866516250991
0.383991172923  - val loss
0.210683836354  - final_loss
Inside Plateau 2



5  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from 0.21068 to 0.20803, saving model to ./log/log_normalisation/cnn/log_normalisation-weights-0.20803.h5
Epoch 00000: val_loss improved from 0.21068 to 0.20803, saving model to ./log/log_normalisation/cnn/log_normalisation_best_weights.h5
37s - loss: 0.3098 - acc: 0.8755 - val_loss: 0.2080 - val_acc: 0.9449
(180000,) (180000,)
86506 3494
23011 66989

FA FR TA TR 0.0388222222222 0.255677777778 0.744322222222 0.961177777778

VALIDATION DATA
0.944944444444 0.208028092093
(18000,) (18000,)
15775 624
367 1234

FA FR TA TR 0.0380511006769 0.229231730169 0.770768269831 0.961948899323
0.208028092093  - val loss
0.210683836354  - final_loss
Validation Loss decreased. Great work



6  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
37s - loss: 0.3072 - acc: 0.8780 - val_loss: 0.2365 - val_acc: 0.9396
(180000,) (180000,)
84794 5206
16967 73033

FA FR TA TR 0.0578444444444 0.188522222222 0.811477777778 0.942155555556

VALIDATION DATA
0.939555555556 0.236461663074
(18000,) (18000,)
15604 795
293 1308

FA FR TA TR 0.0484785657662 0.183010618364 0.816989381636 0.951521434234
0.236461663074  - val loss
0.208028092093  - final_loss
Inside Plateau 1



6  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
37s - loss: 0.3047 - acc: 0.8796 - val_loss: 0.3231 - val_acc: 0.9103
(180000,) (180000,)
80485 9515
9773 80227

FA FR TA TR 0.105722222222 0.108588888889 0.891411111111 0.894277777778

VALIDATION DATA
0.910333333333 0.32307086934
(18000,) (18000,)
14924 1475
139 1462

FA FR TA TR 0.0899445088115 0.0868207370394 0.913179262961 0.910055491188
0.32307086934  - val loss
0.208028092093  - final_loss
Inside Plateau 2



6  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
37s - loss: 0.3028 - acc: 0.8802 - val_loss: 0.2134 - val_acc: 0.9437
(180000,) (180000,)
85859 4141
20220 69780

FA FR TA TR 0.0460111111111 0.224666666667 0.775333333333 0.953988888889

VALIDATION DATA
0.943722222222 0.213392691427
(18000,) (18000,)
15731 668
345 1256

FA FR TA TR 0.0407341911092 0.215490318551 0.784509681449 0.959265808891
0.213392691427  - val loss
0.208028092093  - final_loss
Inside Plateau 3



6  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
37s - loss: 0.3007 - acc: 0.8821 - val_loss: 0.2386 - val_acc: 0.9374
(180000,) (180000,)
84729 5271
15686 74314

FA FR TA TR 0.0585666666667 0.174288888889 0.825711111111 0.941433333333

VALIDATION DATA
0.937444444444 0.23859921915
(18000,) (18000,)
15512 887
239 1362

FA FR TA TR 0.0540886639429 0.149281698938 0.850718301062 0.945911336057
0.23859921915  - val loss
0.208028092093  - final_loss
Reducing the learning rate by half



6  iteration
0.005  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
37s - loss: 0.2909 - acc: 0.8878 - val_loss: 0.2876 - val_acc: 0.9233
(180000,) (180000,)
82822 7178
12086 77914

FA FR TA TR 0.0797555555556 0.134288888889 0.865711111111 0.920244444444

VALIDATION DATA
0.923333333333 0.287596613818
(18000,) (18000,)
15187 1212
168 1433

FA FR TA TR 0.0739069455455 0.10493441599 0.89506558401 0.926093054455
0.287596613818  - val loss
0.208028092093  - final_loss
Inside Plateau 1



6  iteration
0.005  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
37s - loss: 0.2901 - acc: 0.8879 - val_loss: 0.3138 - val_acc: 0.9118
(180000,) (180000,)
81606 8394
10557 79443

FA FR TA TR 0.0932666666667 0.1173 0.8827 0.906733333333

VALIDATION DATA
0.911833333333 0.3138217779
(18000,) (18000,)
14947 1452
135 1466

FA FR TA TR 0.0885419842673 0.0843222985634 0.915677701437 0.911458015733
0.3138217779  - val loss
0.208028092093  - final_loss
Inside Plateau 2



6  iteration
0.005  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
37s - loss: 0.2893 - acc: 0.8884 - val_loss: 0.2207 - val_acc: 0.9407
(180000,) (180000,)
85118 4882
17488 72512

FA FR TA TR 0.0542444444444 0.194311111111 0.805688888889 0.945755555556

VALIDATION DATA
0.940722222222 0.22067517466
(18000,) (18000,)
15626 773
294 1307

FA FR TA TR 0.04713702055 0.183635227983 0.816364772017 0.95286297945
0.22067517466  - val loss
0.208028092093  - final_loss
Inside Plateau 3



6  iteration
0.005  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
37s - loss: 0.2885 - acc: 0.8882 - val_loss: 0.2609 - val_acc: 0.9310
(180000,) (180000,)
83717 6283
13519 76481

FA FR TA TR 0.0698111111111 0.150211111111 0.849788888889 0.930188888889

VALIDATION DATA
0.931 0.260887757036
(18000,) (18000,)
15359 1040
202 1399

FA FR TA TR 0.0634185011281 0.126171143036 0.873828856964 0.936581498872
0.260887757036  - val loss
0.208028092093  - final_loss
Reducing the learning rate by half



6  iteration
0.0025  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
37s - loss: 0.2845 - acc: 0.8908 - val_loss: 0.3610 - val_acc: 0.8869
(180000,) (180000,)
78199 11801
7349 82651

FA FR TA TR 0.131122222222 0.0816555555556 0.918344444444 0.868877777778

VALIDATION DATA
0.886888888889 0.361027737432
(18000,) (18000,)
14450 1949
87 1514

FA FR TA TR 0.118848710287 0.054341036852 0.945658963148 0.881151289713
0.361027737432  - val loss
0.208028092093  - final_loss
Inside Plateau 1



6  iteration
0.0025  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
37s - loss: 0.2845 - acc: 0.8908 - val_loss: 0.2829 - val_acc: 0.9232
(180000,) (180000,)
82640 7360
11616 78384

FA FR TA TR 0.0817777777778 0.129066666667 0.870933333333 0.918222222222

VALIDATION DATA
0.923222222222 0.282937734869
(18000,) (18000,)
15182 1217
165 1436

FA FR TA TR 0.0742118421855 0.103060587133 0.896939412867 0.925788157815
0.282937734869  - val loss
0.208028092093  - final_loss
Inside Plateau 2



6  iteration
0.0025  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
37s - loss: 0.2839 - acc: 0.8909 - val_loss: 0.3253 - val_acc: 0.9057
(180000,) (180000,)
79715 10285
8514 81486

FA FR TA TR 0.114277777778 0.0946 0.9054 0.885722222222

VALIDATION DATA
0.905666666667 0.325265176773
(18000,) (18000,)
14814 1585
113 1488

FA FR TA TR 0.0966522348924 0.0705808869457 0.929419113054 0.903347765108
0.325265176773  - val loss
0.208028092093  - final_loss
Inside Plateau 3



6  iteration
0.0025  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
37s - loss: 0.2836 - acc: 0.8911 - val_loss: 0.2754 - val_acc: 0.9282
(180000,) (180000,)
82793 7207
11778 78222

FA FR TA TR 0.0800777777778 0.130866666667 0.869133333333 0.919922222222

VALIDATION DATA
0.928222222222 0.275415849434
(18000,) (18000,)
15277 1122
170 1431

FA FR TA TR 0.0684188060248 0.106183635228 0.893816364772 0.931581193975
0.275415849434  - val loss
0.208028092093  - final_loss
Reducing the learning rate by half



6  iteration
0.00125  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
37s - loss: 0.2821 - acc: 0.8924 - val_loss: 0.2818 - val_acc: 0.9235
(180000,) (180000,)
82499 7501
11389 78611

FA FR TA TR 0.0833444444444 0.126544444444 0.873455555556 0.916655555556

VALIDATION DATA
0.9235 0.281830169638
(18000,) (18000,)
15180 1219
158 1443

FA FR TA TR 0.0743338008415 0.0986883198001 0.9013116802 0.925666199158
0.281830169638  - val loss
0.208028092093  - final_loss
Inside Plateau 1



6  iteration
0.00125  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
37s - loss: 0.2822 - acc: 0.8920 - val_loss: 0.2875 - val_acc: 0.9226
(180000,) (180000,)
82293 7707
11153 78847

FA FR TA TR 0.0856333333333 0.123922222222 0.876077777778 0.914366666667

VALIDATION DATA
0.922555555556 0.28751953182
(18000,) (18000,)
15151 1248
146 1455

FA FR TA TR 0.0761022013537 0.0911930043723 0.908806995628 0.923897798646
0.28751953182  - val loss
0.208028092093  - final_loss
Inside Plateau 2



6  iteration
0.00125  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
37s - loss: 0.2816 - acc: 0.8923 - val_loss: 0.2695 - val_acc: 0.9279
(180000,) (180000,)
82991 7009
12151 77849

FA FR TA TR 0.0778777777778 0.135011111111 0.864988888889 0.922122222222

VALIDATION DATA
0.927944444444 0.269482186331
(18000,) (18000,)
15276 1123
174 1427

FA FR TA TR 0.0684797853528 0.108682073704 0.891317926296 0.931520214647
0.269482186331  - val loss
0.208028092093  - final_loss
Inside Plateau 3



6  iteration
0.00125  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
37s - loss: 0.2816 - acc: 0.8920 - val_loss: 0.3112 - val_acc: 0.9119
(180000,) (180000,)
80709 9291
9328 80672

FA FR TA TR 0.103233333333 0.103644444444 0.896355555556 0.896766666667

VALIDATION DATA
0.911944444444 0.311152916206
(18000,) (18000,)
14936 1463
122 1479

FA FR TA TR 0.0892127568754 0.0762023735166 0.923797626483 0.910787243125
0.311152916206  - val loss
0.208028092093  - final_loss
Reducing the learning rate by half
   32/18000 [..............................] - ETA: 1s  800/18000 [>.............................] - ETA: 1s 1632/18000 [=>............................] - ETA: 1s 2464/18000 [===>..........................] - ETA: 0s 3296/18000 [====>.........................] - ETA: 0s 4128/18000 [=====>........................] - ETA: 0s 4928/18000 [=======>......................] - ETA: 0s 5760/18000 [========>.....................] - ETA: 0s 6560/18000 [=========>....................] - ETA: 0s 7392/18000 [===========>..................] - ETA: 0s 8160/18000 [============>.................] - ETA: 0s 8960/18000 [=============>................] - ETA: 0s 9696/18000 [===============>..............] - ETA: 0s10496/18000 [================>.............] - ETA: 0s11296/18000 [=================>............] - ETA: 0s12032/18000 [===================>..........] - ETA: 0s12864/18000 [====================>.........] - ETA: 0s13696/18000 [=====================>........] - ETA: 0s14528/18000 [=======================>......] - ETA: 0s15296/18000 [========================>.....] - ETA: 0s16096/18000 [=========================>....] - ETA: 0s16896/18000 [===========================>..] - ETA: 0s17696/18000 [============================>.] - ETA: 0s
ROC AREA:  0.961882225798
(18000,) (18000,)
('\n\nTraining images normalised successfully, shape is ', (18, 1, 584, 565))

train images/masks shape:
(18, 1, 565, 565)
train images range (min-max): 0.0 - 1.0
train masks are within 0-1

positive patches per full image: 5000
negative patches per full image: 5000
('\n\nTraining patches normalised successfully, shape is ', (180000, 1, 27, 27))

train PATCHES images/masks shape:
(180000, 1, 27, 27)
train PATCHES images range (min-max): 0.0 - 1.0
('\n\nTraining images normalised successfully, shape is ', (2, 1, 584, 565))

train images/masks shape:
(2, 1, 565, 565)
train images range (min-max): 0.0 - 1.0
train masks are within 0-1

patches per full image: 9000
('\n\nTraining patches normalised successfully, shape is ', (18000, 1, 27, 27))

train PATCHES images/masks shape:
(18000, 1, 27, 27)
train PATCHES images range (min-max): 0.0 - 1.0
Check: final output of the network:
(None, 2)
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 1, 27, 27)         0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 32, 27, 27)        320       
_________________________________________________________________
dropout_1 (Dropout)          (None, 32, 27, 27)        0         
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 32, 13, 13)        0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 5408)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 2)                 10818     
_________________________________________________________________
activation_1 (Activation)    (None, 2)                 0         
=================================================================
Total params: 11,138
Trainable params: 11,138
Non-trainable params: 0
_________________________________________________________________



1  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from inf to 0.42565, saving model to ./log/log_balanced/log_normalisation/cnn/log_normalisation-weights-0.42565.h5
Epoch 00000: val_loss improved from inf to 0.42565, saving model to ./log/log_balanced/log_normalisation/cnn/log_normalisation_best_weights.h5
39s - loss: 0.5621 - acc: 0.6819 - val_loss: 0.4257 - val_acc: 0.9108
(180000,) (180000,)
83111 6889
31161 58839

FA FR TA TR 0.0765444444444 0.346233333333 0.653766666667 0.923455555556

VALIDATION DATA
0.910833333333 0.425652017805
(18000,) (18000,)
15409 990
615 986

FA FR TA TR 0.0603695347277 0.384134915678 0.615865084322 0.939630465272
0.425652017805  - val loss
999999  - final_loss
Validation Loss decreased. Great work



2  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
38s - loss: 0.4626 - acc: 0.7910 - val_loss: 0.4276 - val_acc: 0.8711
(180000,) (180000,)
77475 12525
18973 71027

FA FR TA TR 0.139166666667 0.210811111111 0.789188888889 0.860833333333

VALIDATION DATA
0.871055555556 0.427590300481
(18000,) (18000,)
14418 1981
340 1261

FA FR TA TR 0.120800048783 0.212367270456 0.787632729544 0.879199951217
0.427590300481  - val loss
0.425652017805  - final_loss
Inside Plateau 1



2  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from 0.42565 to 0.29346, saving model to ./log/log_balanced/log_normalisation/cnn/log_normalisation-weights-0.29346.h5
Epoch 00000: val_loss improved from 0.42565 to 0.29346, saving model to ./log/log_balanced/log_normalisation/cnn/log_normalisation_best_weights.h5
39s - loss: 0.4268 - acc: 0.8123 - val_loss: 0.2935 - val_acc: 0.9349
(180000,) (180000,)
85812 4188
33401 56599

FA FR TA TR 0.0465333333333 0.371122222222 0.628877777778 0.953466666667

VALIDATION DATA
0.934888888889 0.293456590838
(18000,) (18000,)
15919 480
692 909

FA FR TA TR 0.0292700774437 0.43222985634 0.56777014366 0.970729922556
0.293456590838  - val loss
0.425652017805  - final_loss
Validation Loss decreased. Great work



3  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
41s - loss: 0.4042 - acc: 0.8258 - val_loss: 0.3838 - val_acc: 0.8854
(180000,) (180000,)
80379 9621
17730 72270

FA FR TA TR 0.1069 0.197 0.803 0.8931

VALIDATION DATA
0.885444444444 0.383765275637
(18000,) (18000,)
14603 1796
266 1335

FA FR TA TR 0.109518873102 0.166146158651 0.833853841349 0.890481126898
0.383765275637  - val loss
0.293456590838  - final_loss
Inside Plateau 1



3  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
41s - loss: 0.3834 - acc: 0.8359 - val_loss: 0.5223 - val_acc: 0.7196
(180000,) (180000,)
64077 25923
5297 84703

FA FR TA TR 0.288033333333 0.0588555555556 0.941144444444 0.711966666667

VALIDATION DATA
0.719611111111 0.522328669495
(18000,) (18000,)
11419 4980
67 1534

FA FR TA TR 0.303677053479 0.0418488444722 0.958151155528 0.696322946521
0.522328669495  - val loss
0.293456590838  - final_loss
Inside Plateau 2



3  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from 0.29346 to 0.23126, saving model to ./log/log_balanced/log_normalisation/cnn/log_normalisation-weights-0.23126.h5
Epoch 00000: val_loss improved from 0.29346 to 0.23126, saving model to ./log/log_balanced/log_normalisation/cnn/log_normalisation_best_weights.h5
41s - loss: 0.3672 - acc: 0.8450 - val_loss: 0.2313 - val_acc: 0.9444
(180000,) (180000,)
87240 2760
32941 57059

FA FR TA TR 0.0306666666667 0.366011111111 0.633988888889 0.969333333333

VALIDATION DATA
0.944444444444 0.231260818839
(18000,) (18000,)
16066 333
667 934

FA FR TA TR 0.0203061162266 0.416614615865 0.583385384135 0.979693883773
0.231260818839  - val loss
0.293456590838  - final_loss
Validation Loss decreased. Great work



4  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
40s - loss: 0.3510 - acc: 0.8533 - val_loss: 0.3199 - val_acc: 0.9227
(180000,) (180000,)
82801 7199
17185 72815

FA FR TA TR 0.0799888888889 0.190944444444 0.809055555556 0.920011111111

VALIDATION DATA
0.922666666667 0.31987998724
(18000,) (18000,)
15313 1086
306 1295

FA FR TA TR 0.0662235502165 0.19113054341 0.80886945659 0.933776449784
0.31987998724  - val loss
0.231260818839  - final_loss
Inside Plateau 1



4  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
41s - loss: 0.3412 - acc: 0.8592 - val_loss: 0.3197 - val_acc: 0.9165
(180000,) (180000,)
82977 7023
15925 74075

FA FR TA TR 0.0780333333333 0.176944444444 0.823055555556 0.921966666667

VALIDATION DATA
0.9165 0.319670035495
(18000,) (18000,)
15131 1268
235 1366

FA FR TA TR 0.0773217879139 0.146783260462 0.853216739538 0.922678212086
0.319670035495  - val loss
0.231260818839  - final_loss
Inside Plateau 2



4  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
41s - loss: 0.3311 - acc: 0.8645 - val_loss: 0.6512 - val_acc: 0.5702
(180000,) (180000,)
47250 42750
1619 88381

FA FR TA TR 0.475 0.0179888888889 0.982011111111 0.525

VALIDATION DATA
0.570222222222 0.651165307204
(18000,) (18000,)
8682 7717
19 1582

FA FR TA TR 0.470577474236 0.0118675827608 0.988132417239 0.529422525764
0.651165307204  - val loss
0.231260818839  - final_loss
Inside Plateau 3



4  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from 0.23126 to 0.21074, saving model to ./log/log_balanced/log_normalisation/cnn/log_normalisation-weights-0.21074.h5
Epoch 00000: val_loss improved from 0.23126 to 0.21074, saving model to ./log/log_balanced/log_normalisation/cnn/log_normalisation_best_weights.h5
41s - loss: 0.3235 - acc: 0.8687 - val_loss: 0.2107 - val_acc: 0.9443
(180000,) (180000,)
86629 3371
24907 65093

FA FR TA TR 0.0374555555556 0.276744444444 0.723255555556 0.962544444444

VALIDATION DATA
0.944277777778 0.210743002892
(18000,) (18000,)
15833 566
437 1164

FA FR TA TR 0.0345142996524 0.272954403498 0.727045596502 0.965485700348
0.210743002892  - val loss
0.231260818839  - final_loss
Validation Loss decreased. Great work



5  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
41s - loss: 0.3183 - acc: 0.8717 - val_loss: 0.2484 - val_acc: 0.9371
(180000,) (180000,)
84502 5498
17239 72761

FA FR TA TR 0.0610888888889 0.191544444444 0.808455555556 0.938911111111

VALIDATION DATA
0.937111111111 0.248430561781
(18000,) (18000,)
15568 831
301 1300

FA FR TA TR 0.0506738215745 0.188007495315 0.811992504685 0.949326178426
0.248430561781  - val loss
0.210743002892  - final_loss
Inside Plateau 1



5  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
41s - loss: 0.3134 - acc: 0.8738 - val_loss: 0.3838 - val_acc: 0.8730
(180000,) (180000,)
77178 12822
7502 82498

FA FR TA TR 0.142466666667 0.0833555555556 0.916644444444 0.857533333333

VALIDATION DATA
0.873 0.383794175122
(18000,) (18000,)
14211 2188
98 1503

FA FR TA TR 0.133422769681 0.0612117426608 0.938788257339 0.866577230319
0.383794175122  - val loss
0.210743002892  - final_loss
Inside Plateau 2



5  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from 0.21074 to 0.20819, saving model to ./log/log_balanced/log_normalisation/cnn/log_normalisation-weights-0.20819.h5
Epoch 00000: val_loss improved from 0.21074 to 0.20819, saving model to ./log/log_balanced/log_normalisation/cnn/log_normalisation_best_weights.h5
42s - loss: 0.3098 - acc: 0.8754 - val_loss: 0.2082 - val_acc: 0.9451
(180000,) (180000,)
86507 3493
23023 66977

FA FR TA TR 0.0388111111111 0.255811111111 0.744188888889 0.961188888889

VALIDATION DATA
0.945111111111 0.208192314545
(18000,) (18000,)
15778 621
367 1234

FA FR TA TR 0.0378681626928 0.229231730169 0.770768269831 0.962131837307
0.208192314545  - val loss
0.210743002892  - final_loss
Validation Loss decreased. Great work



6  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
42s - loss: 0.3072 - acc: 0.8780 - val_loss: 0.2370 - val_acc: 0.9395
(180000,) (180000,)
84779 5221
16903 73097

FA FR TA TR 0.0580111111111 0.187811111111 0.812188888889 0.941988888889

VALIDATION DATA
0.9395 0.236988372498
(18000,) (18000,)
15601 798
291 1310

FA FR TA TR 0.0486615037502 0.181761399126 0.818238600874 0.95133849625
0.236988372498  - val loss
0.208192314545  - final_loss
Inside Plateau 1



6  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
41s - loss: 0.3047 - acc: 0.8795 - val_loss: 0.3230 - val_acc: 0.9105
(180000,) (180000,)
80491 9509
9770 80230

FA FR TA TR 0.105655555556 0.108555555556 0.891444444444 0.894344444444

VALIDATION DATA
0.9105 0.322969747384
(18000,) (18000,)
14926 1473
138 1463

FA FR TA TR 0.0898225501555 0.0861961274204 0.91380387258 0.910177449845
0.322969747384  - val loss
0.208192314545  - final_loss
Inside Plateau 2



6  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
42s - loss: 0.3028 - acc: 0.8803 - val_loss: 0.2133 - val_acc: 0.9436
(180000,) (180000,)
85861 4139
20246 69754

FA FR TA TR 0.0459888888889 0.224955555556 0.775044444444 0.954011111111

VALIDATION DATA
0.943611111111 0.213336853663
(18000,) (18000,)
15729 670
345 1256

FA FR TA TR 0.0408561497652 0.215490318551 0.784509681449 0.959143850235
0.213336853663  - val loss
0.208192314545  - final_loss
Inside Plateau 3



6  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
41s - loss: 0.3008 - acc: 0.8820 - val_loss: 0.2384 - val_acc: 0.9373
(180000,) (180000,)
84726 5274
15681 74319

FA FR TA TR 0.0586 0.174233333333 0.825766666667 0.9414

VALIDATION DATA
0.937333333333 0.238378845427
(18000,) (18000,)
15511 888
240 1361

FA FR TA TR 0.0541496432709 0.149906308557 0.850093691443 0.945850356729
0.238378845427  - val loss
0.208192314545  - final_loss
Reducing the learning rate by half



6  iteration
0.005  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
42s - loss: 0.2909 - acc: 0.8878 - val_loss: 0.2874 - val_acc: 0.9234
(180000,) (180000,)
82829 7171
12098 77902

FA FR TA TR 0.0796777777778 0.134422222222 0.865577777778 0.920322222222

VALIDATION DATA
0.923388888889 0.287373770078
(18000,) (18000,)
15188 1211
168 1433

FA FR TA TR 0.0738459662175 0.10493441599 0.89506558401 0.926154033783
0.287373770078  - val loss
0.208192314545  - final_loss
Inside Plateau 1



6  iteration
0.005  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
