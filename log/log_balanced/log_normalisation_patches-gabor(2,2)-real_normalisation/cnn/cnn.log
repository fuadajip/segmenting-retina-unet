
train images/masks shape:
(18, 1, 565, 565)
train images range (min-max): 0.0 - 1.0
train masks are within 0-1

positive patches per full image: 5000
negative patches per full image: 5000

train PATCHES images/masks shape:
(180000, 4, 27, 27)
train PATCHES images range (min-max): -8.60396209549 - 8.08247463464

train images/masks shape:
(2, 1, 565, 565)
train images range (min-max): 0.0 - 1.0
train masks are within 0-1

patches per full image: 9000

train PATCHES images/masks shape:
(18000, 4, 27, 27)
train PATCHES images range (min-max): -8.11529146679 - 8.15575393665
Check: final output of the network:
(None, 2)
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 4, 27, 27)         0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 32, 27, 27)        1184      
_________________________________________________________________
dropout_1 (Dropout)          (None, 32, 27, 27)        0         
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 32, 13, 13)        0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 5408)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 2)                 10818     
_________________________________________________________________
activation_1 (Activation)    (None, 2)                 0         
=================================================================
Total params: 12,002
Trainable params: 12,002
Non-trainable params: 0
_________________________________________________________________



1  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from inf to 0.48866, saving model to ./log/log_balanced/log_normalisation_patches-gabor_normalisation/cnn/log_normalisation_patches-gabor_normalisation-weights-0.48866.h5
Epoch 00000: val_loss improved from inf to 0.48866, saving model to ./log/log_balanced/log_normalisation_patches-gabor_normalisation/cnn/log_normalisation_patches-gabor_normalisation_best_weights.h5
97s - loss: 0.5914 - acc: 0.6721 - val_loss: 0.4887 - val_acc: 0.8056
(180000,) (180000,)
74669 15331
38126 51874

FA FR TA TR 0.170344444444 0.423622222222 0.576377777778 0.829655555556

VALIDATION DATA
0.805555555556 0.488658791595
(18000,) (18000,)
13489 2910
590 1011

FA FR TA TR 0.177449844503 0.368519675203 0.631480324797 0.822550155497
0.488658791595  - val loss
999999  - final_loss
Validation Loss decreased. Great work



2  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
39s - loss: 0.5501 - acc: 0.7067 - val_loss: 0.5154 - val_acc: 0.7544
(180000,) (180000,)
69069 20931
29267 60733

FA FR TA TR 0.232566666667 0.325188888889 0.674811111111 0.767433333333

VALIDATION DATA
0.754444444444 0.515366666953
(18000,) (18000,)
12446 3953
467 1134

FA FR TA TR 0.241051283615 0.291692692067 0.708307307933 0.758948716385
0.515366666953  - val loss
0.488658791595  - final_loss
Inside Plateau 1



2  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
39s - loss: 0.5375 - acc: 0.7168 - val_loss: 0.5071 - val_acc: 0.7564
(180000,) (180000,)
69312 20688
27672 62328

FA FR TA TR 0.229866666667 0.307466666667 0.692533333333 0.770133333333

VALIDATION DATA
0.756444444444 0.507114324835
(18000,) (18000,)
12447 3952
432 1169

FA FR TA TR 0.240990304287 0.269831355403 0.730168644597 0.759009695713
0.507114324835  - val loss
0.488658791595  - final_loss
Inside Plateau 2



2  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
39s - loss: 0.5315 - acc: 0.7223 - val_loss: 0.5561 - val_acc: 0.7216
(180000,) (180000,)
64721 25279
22574 67426

FA FR TA TR 0.280877777778 0.250822222222 0.749177777778 0.719122222222

VALIDATION DATA
0.721555555556 0.55607172219
(18000,) (18000,)
11760 4639
373 1228

FA FR TA TR 0.282883102628 0.232979387883 0.767020612117 0.717116897372
0.55607172219  - val loss
0.488658791595  - final_loss
Inside Plateau 3



2  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
39s - loss: 0.5267 - acc: 0.7251 - val_loss: 0.5282 - val_acc: 0.7520
(180000,) (180000,)
67977 22023
25443 64557

FA FR TA TR 0.2447 0.2827 0.7173 0.7553

VALIDATION DATA
0.752 0.528233694235
(18000,) (18000,)
12314 4085
379 1222

FA FR TA TR 0.249100554912 0.236727045597 0.763272954403 0.750899445088
0.528233694235  - val loss
0.488658791595  - final_loss
Reducing the learning rate by half



2  iteration
0.005  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from 0.48866 to 0.39973, saving model to ./log/log_balanced/log_normalisation_patches-gabor_normalisation/cnn/log_normalisation_patches-gabor_normalisation-weights-0.39973.h5
Epoch 00000: val_loss improved from 0.48866 to 0.39973, saving model to ./log/log_balanced/log_normalisation_patches-gabor_normalisation/cnn/log_normalisation_patches-gabor_normalisation_best_weights.h5
40s - loss: 0.5133 - acc: 0.7363 - val_loss: 0.3997 - val_acc: 0.8548
(180000,) (180000,)
79766 10234
38067 51933

FA FR TA TR 0.113711111111 0.422966666667 0.577033333333 0.886288888889

VALIDATION DATA
0.854777777778 0.399725770288
(18000,) (18000,)
14378 2021
593 1008

FA FR TA TR 0.123239221904 0.37039350406 0.62960649594 0.876760778096
0.399725770288  - val loss
0.488658791595  - final_loss
Validation Loss decreased. Great work



3  iteration
0.005  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
40s - loss: 0.5115 - acc: 0.7369 - val_loss: 0.4322 - val_acc: 0.8286
(180000,) (180000,)
76728 13272
33535 56465

FA FR TA TR 0.147466666667 0.372611111111 0.627388888889 0.852533333333

VALIDATION DATA
0.828611111111 0.432175644398
(18000,) (18000,)
13834 2565
520 1081

FA FR TA TR 0.15641197634 0.324797001874 0.675202998126 0.84358802366
0.432175644398  - val loss
0.399725770288  - final_loss
Inside Plateau 1



3  iteration
0.005  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
40s - loss: 0.5094 - acc: 0.7399 - val_loss: 0.4738 - val_acc: 0.7873
(180000,) (180000,)
71932 18068
27085 62915

FA FR TA TR 0.200755555556 0.300944444444 0.699055555556 0.799244444444

VALIDATION DATA
0.787333333333 0.473784716129
(18000,) (18000,)
13021 3378
450 1151

FA FR TA TR 0.20598817001 0.281074328545 0.718925671455 0.79401182999
0.473784716129  - val loss
0.399725770288  - final_loss
Inside Plateau 2



3  iteration
0.005  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
39s - loss: 0.5080 - acc: 0.7401 - val_loss: 0.5734 - val_acc: 0.6906
(180000,) (180000,)
61075 28925
17597 72403

FA FR TA TR 0.321388888889 0.195522222222 0.804477777778 0.678611111111

VALIDATION DATA
0.690611111111 0.573370970355
(18000,) (18000,)
11111 5288
281 1320

FA FR TA TR 0.322458686505 0.175515302936 0.824484697064 0.677541313495
0.573370970355  - val loss
0.399725770288  - final_loss
Inside Plateau 3



3  iteration
0.005  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from 0.39973 to 0.39021, saving model to ./log/log_balanced/log_normalisation_patches-gabor_normalisation/cnn/log_normalisation_patches-gabor_normalisation-weights-0.39021.h5
Epoch 00000: val_loss improved from 0.39973 to 0.39021, saving model to ./log/log_balanced/log_normalisation_patches-gabor_normalisation/cnn/log_normalisation_patches-gabor_normalisation_best_weights.h5
41s - loss: 0.5068 - acc: 0.7416 - val_loss: 0.3902 - val_acc: 0.8565
(180000,) (180000,)
78920 11080
36532 53468

FA FR TA TR 0.123111111111 0.405911111111 0.594088888889 0.876888888889

VALIDATION DATA
0.8565 0.390209408813
(18000,) (18000,)
14390 2009
574 1027

FA FR TA TR 0.122507469968 0.358525921299 0.641474078701 0.877492530032
0.390209408813  - val loss
0.399725770288  - final_loss
Validation Loss decreased. Great work



4  iteration
0.005  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
40s - loss: 0.5055 - acc: 0.7424 - val_loss: 0.4217 - val_acc: 0.8336
(180000,) (180000,)
76744 13256
33197 56803

FA FR TA TR 0.147288888889 0.368855555556 0.631144444444 0.852711111111

VALIDATION DATA
0.833555555556 0.421698228995
(18000,) (18000,)
13902 2497
499 1102

FA FR TA TR 0.152265382035 0.311680199875 0.688319800125 0.847734617965
0.421698228995  - val loss
0.390209408813  - final_loss
Inside Plateau 1



4  iteration
0.005  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
41s - loss: 0.5044 - acc: 0.7429 - val_loss: 0.5338 - val_acc: 0.7331
(180000,) (180000,)
65676 24324
20523 69477

FA FR TA TR 0.270266666667 0.228033333333 0.771966666667 0.729733333333

VALIDATION DATA
0.733055555556 0.533825996187
(18000,) (18000,)
11920 4479
326 1275

FA FR TA TR 0.273126410147 0.20362273579 0.79637726421 0.726873589853
0.533825996187  - val loss
0.390209408813  - final_loss
Inside Plateau 2



4  iteration
0.005  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from 0.39021 to 0.33886, saving model to ./log/log_balanced/log_normalisation_patches-gabor_normalisation/cnn/log_normalisation_patches-gabor_normalisation-weights-0.33886.h5
Epoch 00000: val_loss improved from 0.39021 to 0.33886, saving model to ./log/log_balanced/log_normalisation_patches-gabor_normalisation/cnn/log_normalisation_patches-gabor_normalisation_best_weights.h5
40s - loss: 0.5034 - acc: 0.7437 - val_loss: 0.3389 - val_acc: 0.8838
(180000,) (180000,)
83074 6926
43298 46702

FA FR TA TR 0.0769555555556 0.481088888889 0.518911111111 0.923044444444

VALIDATION DATA
0.883833333333 0.33885665618
(18000,) (18000,)
14990 1409
682 919

FA FR TA TR 0.085919873163 0.42598376015 0.57401623985 0.914080126837
0.33885665618  - val loss
0.390209408813  - final_loss
Validation Loss decreased. Great work



5  iteration
0.005  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
41s - loss: 0.5019 - acc: 0.7445 - val_loss: 0.4473 - val_acc: 0.8063
(180000,) (180000,)
74522 15478
28998 61002

FA FR TA TR 0.171977777778 0.3222 0.6778 0.828022222222

VALIDATION DATA
0.806333333333 0.447302620623
(18000,) (18000,)
13366 3033
453 1148

FA FR TA TR 0.184950301848 0.282948157402 0.717051842598 0.815049698152
0.447302620623  - val loss
0.33885665618  - final_loss
Inside Plateau 1



5  iteration
0.005  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
40s - loss: 0.5014 - acc: 0.7441 - val_loss: 0.5141 - val_acc: 0.7408
(180000,) (180000,)
66642 23358
21389 68611

FA FR TA TR 0.259533333333 0.237655555556 0.762344444444 0.740466666667

VALIDATION DATA
0.740833333333 0.51409961764
(18000,) (18000,)
12093 4306
359 1242

FA FR TA TR 0.262576986402 0.224234853217 0.775765146783 0.737423013598
0.51409961764  - val loss
0.33885665618  - final_loss
Inside Plateau 2



5  iteration
0.005  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
40s - loss: 0.5006 - acc: 0.7451 - val_loss: 0.3650 - val_acc: 0.8703
(180000,) (180000,)
81128 8872
38719 51281

FA FR TA TR 0.0985777777778 0.430211111111 0.569788888889 0.901422222222

VALIDATION DATA
0.870277777778 0.365008417182
(18000,) (18000,)
14679 1720
615 986

FA FR TA TR 0.104884444173 0.384134915678 0.615865084322 0.895115555827
0.365008417182  - val loss
0.33885665618  - final_loss
Inside Plateau 3



5  iteration
0.005  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
39s - loss: 0.4997 - acc: 0.7468 - val_loss: 0.3881 - val_acc: 0.8597
(180000,) (180000,)
79685 10315
35661 54339

FA FR TA TR 0.114611111111 0.396233333333 0.603766666667 0.885388888889

VALIDATION DATA
0.859666666667 0.388132991287
(18000,) (18000,)
14433 1966
560 1041

FA FR TA TR 0.119885358863 0.349781386633 0.650218613367 0.880114641137
0.388132991287  - val loss
0.33885665618  - final_loss
Reducing the learning rate by half



5  iteration
0.0025  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
39s - loss: 0.4929 - acc: 0.7519 - val_loss: 0.4639 - val_acc: 0.7950
(180000,) (180000,)
72763 17237
26093 63907

FA FR TA TR 0.191522222222 0.289922222222 0.710077777778 0.808477777778

VALIDATION DATA
0.795 0.463898208088
(18000,) (18000,)
13134 3265
425 1176

FA FR TA TR 0.199097505945 0.26545908807 0.73454091193 0.800902494055
0.463898208088  - val loss
0.33885665618  - final_loss
Inside Plateau 1



5  iteration
0.0025  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
39s - loss: 0.4924 - acc: 0.7518 - val_loss: 0.4276 - val_acc: 0.8206
(180000,) (180000,)
75621 14379
29686 60314

FA FR TA TR 0.159766666667 0.329844444444 0.670155555556 0.840233333333

VALIDATION DATA
0.820555555556 0.42758492989
(18000,) (18000,)
13663 2736
494 1107

FA FR TA TR 0.166839441429 0.30855715178 0.69144284822 0.833160558571
0.42758492989  - val loss
0.33885665618  - final_loss
Inside Plateau 2



5  iteration
0.0025  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
41s - loss: 0.4916 - acc: 0.7508 - val_loss: 0.3900 - val_acc: 0.8559
(180000,) (180000,)
79956 10044
35422 54578

FA FR TA TR 0.1116 0.393577777778 0.606422222222 0.8884

VALIDATION DATA
0.855944444444 0.389951184511
(18000,) (18000,)
14370 2029
564 1037

FA FR TA TR 0.123727056528 0.352279825109 0.647720174891 0.876272943472
0.389951184511  - val loss
0.33885665618  - final_loss
Inside Plateau 3



5  iteration
0.0025  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
41s - loss: 0.4911 - acc: 0.7531 - val_loss: 0.4779 - val_acc: 0.7809
(180000,) (180000,)
71021 18979
24316 65684

FA FR TA TR 0.210877777778 0.270177777778 0.729822222222 0.789122222222

VALIDATION DATA
0.780888888889 0.477942143705
(18000,) (18000,)
12852 3547
397 1204

FA FR TA TR 0.216293676444 0.247970018738 0.752029981262 0.783706323556
0.477942143705  - val loss
0.33885665618  - final_loss
Reducing the learning rate by half



5  iteration
0.00125  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
41s - loss: 0.4875 - acc: 0.7549 - val_loss: 0.5164 - val_acc: 0.7471
(180000,) (180000,)
67764 22236
20953 69047

FA FR TA TR 0.247066666667 0.232811111111 0.767188888889 0.752933333333

VALIDATION DATA
0.747055555556 0.516401921961
(18000,) (18000,)
12192 4207
346 1255

FA FR TA TR 0.256540032929 0.21611492817 0.78388507183 0.743459967071
0.516401921961  - val loss
0.33885665618  - final_loss
Inside Plateau 1



5  iteration
0.00125  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
40s - loss: 0.4871 - acc: 0.7559 - val_loss: 0.4317 - val_acc: 0.8239
(180000,) (180000,)
75767 14233
29401 60599

FA FR TA TR 0.158144444444 0.326677777778 0.673322222222 0.841855555556

VALIDATION DATA
0.823888888889 0.431722510232
(18000,) (18000,)
13704 2695
475 1126

FA FR TA TR 0.164339288981 0.296689569019 0.703310430981 0.835660711019
0.431722510232  - val loss
0.33885665618  - final_loss
Inside Plateau 2



5  iteration
0.00125  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
41s - loss: 0.4865 - acc: 0.7563 - val_loss: 0.4524 - val_acc: 0.8023
(180000,) (180000,)
73791 16209
26711 63289

FA FR TA TR 0.1801 0.296788888889 0.703211111111 0.8199

VALIDATION DATA
0.802277777778 0.452442816416
(18000,) (18000,)
13264 3135
424 1177

FA FR TA TR 0.191170193304 0.264834478451 0.735165521549 0.808829806696
0.452442816416  - val loss
0.33885665618  - final_loss
Inside Plateau 3



5  iteration
0.00125  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
40s - loss: 0.4869 - acc: 0.7561 - val_loss: 0.4518 - val_acc: 0.8080
(180000,) (180000,)
74508 15492
27163 62837

FA FR TA TR 0.172133333333 0.301811111111 0.698188888889 0.827866666667

VALIDATION DATA
0.808 0.451786486626
(18000,) (18000,)
13377 3022
434 1167

FA FR TA TR 0.18427952924 0.271080574641 0.728919425359 0.81572047076
0.451786486626  - val loss
0.33885665618  - final_loss
Reducing the learning rate by half



5  iteration
0.000625  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
40s - loss: 0.4848 - acc: 0.7576 - val_loss: 0.4351 - val_acc: 0.8199
(180000,) (180000,)
75583 14417
28217 61783

FA FR TA TR 0.160188888889 0.313522222222 0.686477777778 0.839811111111

VALIDATION DATA
0.819888888889 0.435129132854
(18000,) (18000,)
13616 2783
459 1142

FA FR TA TR 0.169705469846 0.286695815116 0.713304184884 0.830294530154
0.435129132854  - val loss
0.33885665618  - final_loss
Inside Plateau 1



5  iteration
0.000625  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
40s - loss: 0.4847 - acc: 0.7582 - val_loss: 0.4426 - val_acc: 0.8148
(180000,) (180000,)
74947 15053
27701 62299

FA FR TA TR 0.167255555556 0.307788888889 0.692211111111 0.832744444444

VALIDATION DATA
0.814833333333 0.442590917905
(18000,) (18000,)
13511 2888
445 1156

FA FR TA TR 0.176108299287 0.27795128045 0.72204871955 0.823891700713
0.442590917905  - val loss
0.33885665618  - final_loss
Inside Plateau 2



5  iteration
0.000625  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
52s - loss: 0.4849 - acc: 0.7577 - val_loss: 0.4330 - val_acc: 0.8227
(180000,) (180000,)
75880 14120
28714 61286

FA FR TA TR 0.156888888889 0.319044444444 0.680955555556 0.843111111111

VALIDATION DATA
0.822722222222 0.433042973598
(18000,) (18000,)
13678 2721
470 1131

FA FR TA TR 0.165924751509 0.293566520924 0.706433479076 0.834075248491
0.433042973598  - val loss
0.33885665618  - final_loss
Inside Plateau 3



5  iteration
0.000625  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
53s - loss: 0.4843 - acc: 0.7582 - val_loss: 0.4604 - val_acc: 0.8009
(180000,) (180000,)
73527 16473
25963 64037

FA FR TA TR 0.183033333333 0.288477777778 0.711522222222 0.816966666667

VALIDATION DATA
0.800888888889 0.46037911431
(18000,) (18000,)
13226 3173
411 1190

FA FR TA TR 0.193487407769 0.256714553404 0.743285446596 0.806512592231
0.46037911431  - val loss
0.33885665618  - final_loss
Reducing the learning rate by half
   32/18000 [..............................] - ETA: 1s  672/18000 [>.............................] - ETA: 1s 1344/18000 [=>............................] - ETA: 1s 2048/18000 [==>...........................] - ETA: 1s 2688/18000 [===>..........................] - ETA: 1s 3360/18000 [====>.........................] - ETA: 1s 4032/18000 [=====>........................] - ETA: 1s 4736/18000 [======>.......................] - ETA: 1s 5408/18000 [========>.....................] - ETA: 0s 6112/18000 [=========>....................] - ETA: 0s 6816/18000 [==========>...................] - ETA: 0s 7520/18000 [===========>..................] - ETA: 0s 8224/18000 [============>.................] - ETA: 0s 8928/18000 [=============>................] - ETA: 0s 9664/18000 [===============>..............] - ETA: 0s10336/18000 [================>.............] - ETA: 0s11008/18000 [=================>............] - ETA: 0s11680/18000 [==================>...........] - ETA: 0s12384/18000 [===================>..........] - ETA: 0s13088/18000 [====================>.........] - ETA: 0s13824/18000 [======================>.......] - ETA: 0s14560/18000 [=======================>......] - ETA: 0s15264/18000 [========================>.....] - ETA: 0s15968/18000 [=========================>....] - ETA: 0s16672/18000 [==========================>...] - ETA: 0s17344/18000 [===========================>..] - ETA: 0s
ROC AREA:  0.849643164284
(18000,) (18000,)
