('\n\nTraining images normalised successfully, shape is ', (18, 1, 584, 565))

train images/masks shape:
(18, 1, 565, 565)
train images range (min-max): 0.0 - 1.0
train masks are within 0-1

positive patches per full image: 500
negative patches per full image: 500
('\n\nTraining patches normalised successfully, shape is ', (18000, 32, 27, 27))

train PATCHES images/masks shape:
(18000, 32, 27, 27)
train PATCHES images range (min-max): -13.1525775796 - 12.5848885197
('\n\nTraining images normalised successfully, shape is ', (2, 1, 584, 565))

train images/masks shape:
(2, 1, 565, 565)
train images range (min-max): 0.0 - 1.0
train masks are within 0-1

patches per full image: 9000
('\n\nTraining patches normalised successfully, shape is ', (18000, 32, 27, 27))

train PATCHES images/masks shape:
(18000, 32, 27, 27)
train PATCHES images range (min-max): -12.6309545866 - 12.622778836
Check: final output of the network:
(None, 2)
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 32, 27, 27)        0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 32, 27, 27)        9248      
_________________________________________________________________
dropout_1 (Dropout)          (None, 32, 27, 27)        0         
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 32, 13, 13)        0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 5408)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 2)                 10818     
_________________________________________________________________
activation_1 (Activation)    (None, 2)                 0         
=================================================================
Total params: 20,066
Trainable params: 20,066
Non-trainable params: 0
_________________________________________________________________



1  iteration
0.01  learning rate

TRAIN DATA
Train on 18000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from inf to 0.38413, saving model to ./log/log_balanced/log_normalisation_patches-gabor(4,4)-real-image_normalisation_reduced-dataset(18000)/cnn/log_normalisation_patches-gabor(4,4)-real-image_normalisation_reduced-dataset(18000)-weights-0.38413.h5
Epoch 00000: val_loss improved from inf to 0.38413, saving model to ./log/log_balanced/log_normalisation_patches-gabor(4,4)-real-image_normalisation_reduced-dataset(18000)/cnn/log_normalisation_patches-gabor(4,4)-real-image_normalisation_reduced-dataset(18000)_best_weights.h5
43s - loss: 0.6937 - acc: 0.7666 - val_loss: 0.3841 - val_acc: 0.8516
(18000,) (18000,)
7652 1348
1515 7485

FA FR TA TR 0.149777777778 0.168333333333 0.831666666667 0.850222222222

VALIDATION DATA
0.851611111111 0.384128541231
(18000,) (18000,)
13936 2351
320 1393

FA FR TA TR 0.144348253208 0.186806771745 0.813193228255 0.855651746792
0.384128541231  - val loss
999999  - final_loss
Validation Loss decreased. Great work



2  iteration
0.01  learning rate

TRAIN DATA
Train on 18000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from 0.38413 to 0.32088, saving model to ./log/log_balanced/log_normalisation_patches-gabor(4,4)-real-image_normalisation_reduced-dataset(18000)/cnn/log_normalisation_patches-gabor(4,4)-real-image_normalisation_reduced-dataset(18000)-weights-0.32088.h5
Epoch 00000: val_loss improved from 0.38413 to 0.32088, saving model to ./log/log_balanced/log_normalisation_patches-gabor(4,4)-real-image_normalisation_reduced-dataset(18000)/cnn/log_normalisation_patches-gabor(4,4)-real-image_normalisation_reduced-dataset(18000)_best_weights.h5
33s - loss: 0.3799 - acc: 0.8299 - val_loss: 0.3209 - val_acc: 0.8926
(18000,) (18000,)
8102 898
1763 7237

FA FR TA TR 0.0997777777778 0.195888888889 0.804111111111 0.900222222222

VALIDATION DATA
0.892611111111 0.320883325934
(18000,) (18000,)
14719 1568
365 1348

FA FR TA TR 0.0962731012464 0.213076474022 0.786923525978 0.903726898754
0.320883325934  - val loss
0.384128541231  - final_loss
Validation Loss decreased. Great work



3  iteration
0.01  learning rate

TRAIN DATA
Train on 18000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
10s - loss: 0.3610 - acc: 0.8402 - val_loss: 0.3524 - val_acc: 0.8600
(18000,) (18000,)
7739 1261
1356 7644

FA FR TA TR 0.140111111111 0.150666666667 0.849333333333 0.859888888889

VALIDATION DATA
0.86 0.35238735591
(18000,) (18000,)
14062 2225
295 1418

FA FR TA TR 0.136612021858 0.172212492703 0.827787507297 0.863387978142
0.35238735591  - val loss
0.320883325934  - final_loss
Inside Plateau 1



3  iteration
0.01  learning rate

TRAIN DATA
Train on 18000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from 0.32088 to 0.18124, saving model to ./log/log_balanced/log_normalisation_patches-gabor(4,4)-real-image_normalisation_reduced-dataset(18000)/cnn/log_normalisation_patches-gabor(4,4)-real-image_normalisation_reduced-dataset(18000)-weights-0.18124.h5
Epoch 00000: val_loss improved from 0.32088 to 0.18124, saving model to ./log/log_balanced/log_normalisation_patches-gabor(4,4)-real-image_normalisation_reduced-dataset(18000)/cnn/log_normalisation_patches-gabor(4,4)-real-image_normalisation_reduced-dataset(18000)_best_weights.h5
27s - loss: 0.3555 - acc: 0.8428 - val_loss: 0.1812 - val_acc: 0.9357
(18000,) (18000,)
8764 236
3480 5520

FA FR TA TR 0.0262222222222 0.386666666667 0.613333333333 0.973777777778

VALIDATION DATA
0.935722222222 0.181242896292
(18000,) (18000,)
15848 439
718 995

FA FR TA TR 0.0269540124025 0.419147694104 0.580852305896 0.973045987597
0.181242896292  - val loss
0.320883325934  - final_loss
Validation Loss decreased. Great work



4  iteration
0.01  learning rate

TRAIN DATA
Train on 18000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
9s - loss: 0.3439 - acc: 0.8476 - val_loss: 0.2378 - val_acc: 0.9192
(18000,) (18000,)
8509 491
2392 6608

FA FR TA TR 0.0545555555556 0.265777777778 0.734222222222 0.945444444444

VALIDATION DATA
0.919222222222 0.237817061908
(18000,) (18000,)
15340 947
507 1206

FA FR TA TR 0.0581445324492 0.295971978984 0.704028021016 0.941855467551
0.237817061908  - val loss
0.181242896292  - final_loss
Inside Plateau 1



4  iteration
0.01  learning rate

TRAIN DATA
Train on 18000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
10s - loss: 0.3384 - acc: 0.8520 - val_loss: 0.3027 - val_acc: 0.8941
(18000,) (18000,)
8205 795
1622 7378

FA FR TA TR 0.0883333333333 0.180222222222 0.819777777778 0.911666666667

VALIDATION DATA
0.894055555556 0.302721421017
(18000,) (18000,)
14763 1524
383 1330

FA FR TA TR 0.09357156014 0.223584354933 0.776415645067 0.90642843986
0.302721421017  - val loss
0.181242896292  - final_loss
Inside Plateau 2



4  iteration
0.01  learning rate

TRAIN DATA
Train on 18000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
9s - loss: 0.3304 - acc: 0.8558 - val_loss: 0.3431 - val_acc: 0.8628
(18000,) (18000,)
7801 1199
1335 7665

FA FR TA TR 0.133222222222 0.148333333333 0.851666666667 0.866777777778

VALIDATION DATA
0.862833333333 0.343132680893
(18000,) (18000,)
14132 2155
314 1399

FA FR TA TR 0.132314115552 0.183304144775 0.816695855225 0.867685884448
0.343132680893  - val loss
0.181242896292  - final_loss
Inside Plateau 3



4  iteration
0.01  learning rate

TRAIN DATA
Train on 18000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
9s - loss: 0.3305 - acc: 0.8556 - val_loss: 0.3711 - val_acc: 0.8438
(18000,) (18000,)
7728 1272
1090 7910

FA FR TA TR 0.141333333333 0.121111111111 0.878888888889 0.858666666667

VALIDATION DATA
0.843777777778 0.371119274272
(18000,) (18000,)
13738 2549
263 1450

FA FR TA TR 0.156505188187 0.153531815528 0.846468184472 0.843494811813
0.371119274272  - val loss
0.181242896292  - final_loss
Reducing the learning rate by half



4  iteration
0.005  learning rate

TRAIN DATA
Train on 18000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
9s - loss: 0.3044 - acc: 0.8679 - val_loss: 0.4116 - val_acc: 0.8281
(18000,) (18000,)
7497 1503
733 8267

FA FR TA TR 0.167 0.0814444444444 0.918555555556 0.833

VALIDATION DATA
0.828055555556 0.411586044762
(18000,) (18000,)
13394 2893
202 1511

FA FR TA TR 0.177626327746 0.117921774664 0.882078225336 0.822373672254
0.411586044762  - val loss
0.181242896292  - final_loss
Inside Plateau 1



4  iteration
0.005  learning rate

TRAIN DATA
Train on 18000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
9s - loss: 0.3002 - acc: 0.8691 - val_loss: 0.3449 - val_acc: 0.8696
(18000,) (18000,)
7977 1023
1043 7957

FA FR TA TR 0.113666666667 0.115888888889 0.884111111111 0.886333333333

VALIDATION DATA
0.869555555556 0.34487384359
(18000,) (18000,)
14222 2065
283 1430

FA FR TA TR 0.126788236016 0.165207238762 0.834792761238 0.873211763984
0.34487384359  - val loss
0.181242896292  - final_loss
Inside Plateau 2



4  iteration
0.005  learning rate

TRAIN DATA
Train on 18000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
9s - loss: 0.2984 - acc: 0.8726 - val_loss: 0.2355 - val_acc: 0.9213
(18000,) (18000,)
8539 461
1969 7031

FA FR TA TR 0.0512222222222 0.218777777778 0.781222222222 0.948777777778

VALIDATION DATA
0.921277777778 0.235533158223
(18000,) (18000,)
15317 970
447 1266

FA FR TA TR 0.0595567016639 0.260945709282 0.739054290718 0.940443298336
0.235533158223  - val loss
0.181242896292  - final_loss
Inside Plateau 3



4  iteration
0.005  learning rate

TRAIN DATA
Train on 18000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
9s - loss: 0.2960 - acc: 0.8738 - val_loss: 0.3556 - val_acc: 0.8602
(18000,) (18000,)
7892 1108
946 8054

FA FR TA TR 0.123111111111 0.105111111111 0.894888888889 0.876888888889

VALIDATION DATA
0.860166666667 0.355624266121
(18000,) (18000,)
14028 2259
258 1455

FA FR TA TR 0.138699576349 0.15061295972 0.84938704028 0.861300423651
0.355624266121  - val loss
0.181242896292  - final_loss
Reducing the learning rate by half



4  iteration
0.0025  learning rate

TRAIN DATA
Train on 18000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
9s - loss: 0.2833 - acc: 0.8782 - val_loss: 0.3052 - val_acc: 0.8861
(18000,) (18000,)
8172 828
1198 7802

FA FR TA TR 0.092 0.133111111111 0.866888888889 0.908

VALIDATION DATA
0.886111111111 0.305220046918
(18000,) (18000,)
14554 1733
317 1396

FA FR TA TR 0.106403880395 0.18505545826 0.81494454174 0.893596119605
0.305220046918  - val loss
0.181242896292  - final_loss
Inside Plateau 1



4  iteration
0.0025  learning rate

TRAIN DATA
Train on 18000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
9s - loss: 0.2800 - acc: 0.8801 - val_loss: 0.2768 - val_acc: 0.9038
(18000,) (18000,)
8371 629
1426 7574

FA FR TA TR 0.0698888888889 0.158444444444 0.841555555556 0.930111111111

VALIDATION DATA
0.903833333333 0.276800502274
(18000,) (18000,)
14926 1361
370 1343

FA FR TA TR 0.083563578314 0.215995329831 0.784004670169 0.916436421686
0.276800502274  - val loss
0.181242896292  - final_loss
Inside Plateau 2



4  iteration
0.0025  learning rate

TRAIN DATA
Train on 18000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
9s - loss: 0.2786 - acc: 0.8808 - val_loss: 0.2812 - val_acc: 0.9037
(18000,) (18000,)
8395 605
1432 7568

FA FR TA TR 0.0672222222222 0.159111111111 0.840888888889 0.932777777778

VALIDATION DATA
0.903666666667 0.281208750288
(18000,) (18000,)
14915 1372
362 1351

FA FR TA TR 0.0842389635906 0.211325160537 0.788674839463 0.915761036409
0.281208750288  - val loss
0.181242896292  - final_loss
Inside Plateau 3



4  iteration
0.0025  learning rate

TRAIN DATA
Train on 18000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
9s - loss: 0.2775 - acc: 0.8812 - val_loss: 0.2942 - val_acc: 0.8926
(18000,) (18000,)
8304 696
1289 7711

FA FR TA TR 0.0773333333333 0.143222222222 0.856777777778 0.922666666667

VALIDATION DATA
0.892611111111 0.294175109784
(18000,) (18000,)
14683 1604
329 1384

FA FR TA TR 0.0984834530607 0.192060712201 0.807939287799 0.901516546939
0.294175109784  - val loss
0.181242896292  - final_loss
Reducing the learning rate by half



4  iteration
0.00125  learning rate

TRAIN DATA
Train on 18000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
9s - loss: 0.2710 - acc: 0.8862 - val_loss: 0.3286 - val_acc: 0.8745
(18000,) (18000,)
8087 913
960 8040

FA FR TA TR 0.101444444444 0.106666666667 0.893333333333 0.898555555556

VALIDATION DATA
0.8745 0.328641875651
(18000,) (18000,)
14310 1977
282 1431

FA FR TA TR 0.121385153804 0.164623467601 0.835376532399 0.878614846196
0.328641875651  - val loss
0.181242896292  - final_loss
Inside Plateau 1



4  iteration
0.00125  learning rate

TRAIN DATA
Train on 18000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
9s - loss: 0.2699 - acc: 0.8858 - val_loss: 0.2930 - val_acc: 0.8961
(18000,) (18000,)
8296 704
1221 7779

FA FR TA TR 0.0782222222222 0.135666666667 0.864333333333 0.921777777778

VALIDATION DATA
0.896055555556 0.292988594916
(18000,) (18000,)
14744 1543
328 1385

FA FR TA TR 0.0947381347087 0.191476941039 0.808523058961 0.905261865291
0.292988594916  - val loss
0.181242896292  - final_loss
Inside Plateau 2



4  iteration
0.00125  learning rate

TRAIN DATA
Train on 18000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
9s - loss: 0.2690 - acc: 0.8871 - val_loss: 0.3111 - val_acc: 0.8858
(18000,) (18000,)
8195 805
1071 7929

FA FR TA TR 0.0894444444444 0.119 0.881 0.910555555556

VALIDATION DATA
0.885777777778 0.311059251441
(18000,) (18000,)
14531 1756
300 1413

FA FR TA TR 0.10781604961 0.175131348511 0.824868651489 0.89218395039
0.311059251441  - val loss
0.181242896292  - final_loss
Inside Plateau 3



4  iteration
0.00125  learning rate

TRAIN DATA
Train on 18000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
9s - loss: 0.2690 - acc: 0.8838 - val_loss: 0.3209 - val_acc: 0.8792
(18000,) (18000,)
8124 876
1001 7999

FA FR TA TR 0.0973333333333 0.111222222222 0.888777777778 0.902666666667

VALIDATION DATA
0.879222222222 0.320872519811
(18000,) (18000,)
14409 1878
296 1417

FA FR TA TR 0.115306686314 0.172796263865 0.827203736135 0.884693313686
0.320872519811  - val loss
0.181242896292  - final_loss
Reducing the learning rate by half
   32/18000 [..............................] - ETA: 3s  160/18000 [..............................] - ETA: 6s  416/18000 [..............................] - ETA: 4s  704/18000 [>.............................] - ETA: 4s  960/18000 [>.............................] - ETA: 3s 1248/18000 [=>............................] - ETA: 3s 1536/18000 [=>............................] - ETA: 3s 1824/18000 [==>...........................] - ETA: 3s 2112/18000 [==>...........................] - ETA: 3s 2400/18000 [===>..........................] - ETA: 3s 2688/18000 [===>..........................] - ETA: 3s 2912/18000 [===>..........................] - ETA: 3s 3200/18000 [====>.........................] - ETA: 2s 3488/18000 [====>.........................] - ETA: 2s 3776/18000 [=====>........................] - ETA: 2s 4064/18000 [=====>........................] - ETA: 2s 4352/18000 [======>.......................] - ETA: 2s 4640/18000 [======>.......................] - ETA: 2s 4928/18000 [=======>......................] - ETA: 2s 5216/18000 [=======>......................] - ETA: 2s 5504/18000 [========>.....................] - ETA: 2s 5760/18000 [========>.....................] - ETA: 2s 6016/18000 [=========>....................] - ETA: 2s 6304/18000 [=========>....................] - ETA: 2s 6592/18000 [=========>....................] - ETA: 2s 6880/18000 [==========>...................] - ETA: 2s 7168/18000 [==========>...................] - ETA: 2s 7456/18000 [===========>..................] - ETA: 2s 7744/18000 [===========>..................] - ETA: 1s 8032/18000 [============>.................] - ETA: 1s 8288/18000 [============>.................] - ETA: 1s 8576/18000 [=============>................] - ETA: 1s 8864/18000 [=============>................] - ETA: 1s 9120/18000 [==============>...............] - ETA: 1s 9408/18000 [==============>...............] - ETA: 1s 9696/18000 [===============>..............] - ETA: 1s 9984/18000 [===============>..............] - ETA: 1s10272/18000 [================>.............] - ETA: 1s10560/18000 [================>.............] - ETA: 1s10848/18000 [=================>............] - ETA: 1s11104/18000 [=================>............] - ETA: 1s11392/18000 [=================>............] - ETA: 1s11680/18000 [==================>...........] - ETA: 1s11968/18000 [==================>...........] - ETA: 1s12256/18000 [===================>..........] - ETA: 1s12512/18000 [===================>..........] - ETA: 1s12800/18000 [====================>.........] - ETA: 0s13088/18000 [====================>.........] - ETA: 0s13376/18000 [=====================>........] - ETA: 0s13632/18000 [=====================>........] - ETA: 0s13920/18000 [======================>.......] - ETA: 0s14208/18000 [======================>.......] - ETA: 0s14496/18000 [=======================>......] - ETA: 0s14784/18000 [=======================>......] - ETA: 0s15072/18000 [========================>.....] - ETA: 0s15360/18000 [========================>.....] - ETA: 0s15616/18000 [=========================>....] - ETA: 0s15904/18000 [=========================>....] - ETA: 0s16160/18000 [=========================>....] - ETA: 0s16448/18000 [==========================>...] - ETA: 0s16736/18000 [==========================>...] - ETA: 0s17024/18000 [===========================>..] - ETA: 0s17248/18000 [===========================>..] - ETA: 0s17536/18000 [============================>.] - ETA: 0s17824/18000 [============================>.] - ETA: 0s
ROC AREA:  0.929913678787
(18000,) (18000,)
