
train images/masks shape:
(18, 1, 565, 565)
train images range (min-max): 0.0 - 1.0
train masks are within 0-1

patches per full image: 10000

train PATCHES images/masks shape:
(180000, 1, 27, 27)
train PATCHES images range (min-max): 0.0 - 1.0

train images/masks shape:
(2, 1, 565, 565)
train images range (min-max): 0.0 - 1.0
train masks are within 0-1

patches per full image: 9000

train PATCHES images/masks shape:
(18000, 1, 27, 27)
train PATCHES images range (min-max): 0.0 - 1.0
Check: final output of the network:
(None, 2)
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 1, 27, 27)         0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 32, 27, 27)        320       
_________________________________________________________________
dropout_1 (Dropout)          (None, 32, 27, 27)        0         
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 32, 13, 13)        0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 5408)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 2)                 10818     
_________________________________________________________________
activation_1 (Activation)    (None, 2)                 0         
=================================================================
Total params: 11,138
Trainable params: 11,138
Non-trainable params: 0
_________________________________________________________________



1  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from inf to 0.28533, saving model to ./log/cnn/log-weights-0.28533.h5
Epoch 00000: val_loss improved from inf to 0.28533, saving model to ./log/cnn/log_best_weights.h5
40s - loss: 0.3045 - acc: 0.9006 - val_loss: 0.2853 - val_acc: 0.9076
(180000,) (180000,)
162158 0
17842 0

FA FR TA TR 0.0 1.0 0.0 1.0

VALIDATION DATA
0.907611111111 0.285333914412
(18000,) (18000,)
16337 0
1663 0

FA FR TA TR 0.0 1.0 0.0 1.0
0.285333914412  - val loss
999999  - final_loss
Validation Loss decreased. Great work



2  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from 0.28533 to 0.26312, saving model to ./log/cnn/log-weights-0.26312.h5
Epoch 00000: val_loss improved from 0.28533 to 0.26312, saving model to ./log/cnn/log_best_weights.h5
37s - loss: 0.2746 - acc: 0.9020 - val_loss: 0.2631 - val_acc: 0.9127
(180000,) (180000,)
162040 118
16950 892

FA FR TA TR 0.000727685343924 0.950005604753 0.0499943952472 0.999272314656

VALIDATION DATA
0.912722222222 0.263121850954
(18000,) (18000,)
16326 11
1560 103

FA FR TA TR 0.000673318234682 0.938063740229 0.0619362597715 0.999326681765
0.263121850954  - val loss
0.285333914412  - final_loss
Validation Loss decreased. Great work



3  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from 0.26312 to 0.23648, saving model to ./log/cnn/log-weights-0.23648.h5
Epoch 00000: val_loss improved from 0.26312 to 0.23648, saving model to ./log/cnn/log_best_weights.h5
37s - loss: 0.2428 - acc: 0.9114 - val_loss: 0.2365 - val_acc: 0.9213
(180000,) (180000,)
161636 522
14915 2927

FA FR TA TR 0.00321908262312 0.835948884654 0.164051115346 0.996780917377

VALIDATION DATA
0.921277777778 0.236483783603
(18000,) (18000,)
16295 42
1375 288

FA FR TA TR 0.00257085144151 0.826819001804 0.173180998196 0.997429148558
0.236483783603  - val loss
0.263121850954  - final_loss
Validation Loss decreased. Great work



4  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from 0.23648 to 0.22952, saving model to ./log/cnn/log-weights-0.22952.h5
Epoch 00000: val_loss improved from 0.23648 to 0.22952, saving model to ./log/cnn/log_best_weights.h5
37s - loss: 0.2276 - acc: 0.9184 - val_loss: 0.2295 - val_acc: 0.9263
(180000,) (180000,)
161118 1040
13145 4697

FA FR TA TR 0.00641349794645 0.736744759556 0.263255240444 0.993586502054

VALIDATION DATA
0.926333333333 0.229519496163
(18000,) (18000,)
16237 100
1226 437

FA FR TA TR 0.00612107486075 0.737221888154 0.262778111846 0.993878925139
0.229519496163  - val loss
0.236483783603  - final_loss
Validation Loss decreased. Great work



5  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from 0.22952 to 0.21422, saving model to ./log/cnn/log-weights-0.21422.h5
Epoch 00000: val_loss improved from 0.22952 to 0.21422, saving model to ./log/cnn/log_best_weights.h5
36s - loss: 0.2189 - acc: 0.9222 - val_loss: 0.2142 - val_acc: 0.9254
(180000,) (180000,)
161341 817
13299 4543

FA FR TA TR 0.00503829598293 0.745376078915 0.254623921085 0.994961704017

VALIDATION DATA
0.925444444444 0.214220196797
(18000,) (18000,)
16259 78
1264 399

FA FR TA TR 0.00477443839138 0.760072158749 0.239927841251 0.995225561609
0.214220196797  - val loss
0.229519496163  - final_loss
Validation Loss decreased. Great work



6  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from 0.21422 to 0.21416, saving model to ./log/cnn/log-weights-0.21416.h5
Epoch 00000: val_loss improved from 0.21422 to 0.21416, saving model to ./log/cnn/log_best_weights.h5
36s - loss: 0.2130 - acc: 0.9246 - val_loss: 0.2142 - val_acc: 0.9313
(180000,) (180000,)
160916 1242
11991 5851

FA FR TA TR 0.00765919658605 0.672065911893 0.327934088107 0.992340803414

VALIDATION DATA
0.931333333333 0.214159658763
(18000,) (18000,)
16230 107
1129 534

FA FR TA TR 0.006549550101 0.678893565845 0.321106434155 0.993450449899
0.214159658763  - val loss
0.214220196797  - final_loss
Validation Loss decreased. Great work



7  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from 0.21416 to 0.20385, saving model to ./log/cnn/log-weights-0.20385.h5
Epoch 00000: val_loss improved from 0.21416 to 0.20385, saving model to ./log/cnn/log_best_weights.h5
36s - loss: 0.2083 - acc: 0.9268 - val_loss: 0.2038 - val_acc: 0.9267
(180000,) (180000,)
161589 569
13483 4359

FA FR TA TR 0.0035089233957 0.755688824123 0.244311175877 0.996491076604

VALIDATION DATA
0.926666666667 0.203847870661
(18000,) (18000,)
16297 40
1280 383

FA FR TA TR 0.0024484299443 0.769693325316 0.230306674684 0.997551570056
0.203847870661  - val loss
0.214159658763  - final_loss
Validation Loss decreased. Great work



8  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
36s - loss: 0.2049 - acc: 0.9279 - val_loss: 0.2267 - val_acc: 0.9396
(180000,) (180000,)
159621 2537
9808 8034

FA FR TA TR 0.0156452348944 0.549714157606 0.450285842394 0.984354765106

VALIDATION DATA
0.939555555556 0.226710902638
(18000,) (18000,)
16125 212
876 787

FA FR TA TR 0.0129766787048 0.526758869513 0.473241130487 0.987023321295
0.226710902638  - val loss
0.203847870661  - final_loss
Inside Plateau 1



8  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from 0.20385 to 0.19286, saving model to ./log/cnn/log-weights-0.19286.h5
Epoch 00000: val_loss improved from 0.20385 to 0.19286, saving model to ./log/cnn/log_best_weights.h5
37s - loss: 0.2014 - acc: 0.9295 - val_loss: 0.1929 - val_acc: 0.9312
(180000,) (180000,)
161306 852
12115 5727

FA FR TA TR 0.00525413485613 0.679015805403 0.320984194597 0.994745865144

VALIDATION DATA
0.931166666667 0.192864461886
(18000,) (18000,)
16276 61
1178 485

FA FR TA TR 0.00373385566505 0.708358388455 0.291641611545 0.996266144335
0.192864461886  - val loss
0.203847870661  - final_loss
Validation Loss decreased. Great work



9  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from 0.19286 to 0.18822, saving model to ./log/cnn/log-weights-0.18822.h5
Epoch 00000: val_loss improved from 0.19286 to 0.18822, saving model to ./log/cnn/log_best_weights.h5
37s - loss: 0.1982 - acc: 0.9304 - val_loss: 0.1882 - val_acc: 0.9318
(180000,) (180000,)
161301 857
12001 5841

FA FR TA TR 0.00528496898087 0.672626387176 0.327373612824 0.994715031019

VALIDATION DATA
0.931833333333 0.188218760517
(18000,) (18000,)
16272 65
1162 501

FA FR TA TR 0.00397869865948 0.698737221888 0.301262778112 0.996021301341
0.188218760517  - val loss
0.192864461886  - final_loss
Validation Loss decreased. Great work



10  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from 0.18822 to 0.18771, saving model to ./log/cnn/log-weights-0.18771.h5
Epoch 00000: val_loss improved from 0.18822 to 0.18771, saving model to ./log/cnn/log_best_weights.h5
37s - loss: 0.1952 - acc: 0.9313 - val_loss: 0.1877 - val_acc: 0.9293
(180000,) (180000,)
161398 760
12354 5488

FA FR TA TR 0.00468678696087 0.692411164668 0.307588835332 0.995313213039

VALIDATION DATA
0.929333333333 0.187707032031
(18000,) (18000,)
16274 63
1209 454

FA FR TA TR 0.00385627716227 0.726999398677 0.273000601323 0.996143722838
0.187707032031  - val loss
0.188218760517  - final_loss
Validation Loss decreased. Great work



11  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from 0.18771 to 0.18314, saving model to ./log/cnn/log-weights-0.18314.h5
Epoch 00000: val_loss improved from 0.18771 to 0.18314, saving model to ./log/cnn/log_best_weights.h5
37s - loss: 0.1921 - acc: 0.9323 - val_loss: 0.1831 - val_acc: 0.9392
(180000,) (180000,)
160719 1439
10371 7471

FA FR TA TR 0.0088740611009 0.581268916041 0.418731083959 0.991125938899

VALIDATION DATA
0.939166666667 0.183135555473
(18000,) (18000,)
16222 115
980 683

FA FR TA TR 0.00703923608986 0.589296452195 0.410703547805 0.99296076391
0.183135555473  - val loss
0.187707032031  - final_loss
Validation Loss decreased. Great work



12  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from 0.18314 to 0.17789, saving model to ./log/cnn/log-weights-0.17789.h5
Epoch 00000: val_loss improved from 0.18314 to 0.17789, saving model to ./log/cnn/log_best_weights.h5
37s - loss: 0.1889 - acc: 0.9334 - val_loss: 0.1779 - val_acc: 0.9387
(180000,) (180000,)
160669 1489
10126 7716

FA FR TA TR 0.00918240234833 0.567537271606 0.432462728394 0.990817597652

VALIDATION DATA
0.938666666667 0.17789339443
(18000,) (18000,)
16232 105
999 664

FA FR TA TR 0.00642712860378 0.600721587492 0.399278412508 0.993572871396
0.17789339443  - val loss
0.183135555473  - final_loss
Validation Loss decreased. Great work



13  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from 0.17789 to 0.17351, saving model to ./log/cnn/log-weights-0.17351.h5
Epoch 00000: val_loss improved from 0.17789 to 0.17351, saving model to ./log/cnn/log_best_weights.h5
37s - loss: 0.1863 - acc: 0.9343 - val_loss: 0.1735 - val_acc: 0.9412
(180000,) (180000,)
160707 1451
10003 7839

FA FR TA TR 0.00894806300028 0.560643425625 0.439356574375 0.991051937

VALIDATION DATA
0.941222222222 0.173505320748
(18000,) (18000,)
16235 102
956 707

FA FR TA TR 0.00624349635796 0.574864702345 0.425135297655 0.993756503642
0.173505320748  - val loss
0.17789339443  - final_loss
Validation Loss decreased. Great work



14  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
37s - loss: 0.1837 - acc: 0.9349 - val_loss: 0.1755 - val_acc: 0.9334
(180000,) (180000,)
161539 619
12053 5789

FA FR TA TR 0.00381726464313 0.675540858648 0.324459141352 0.996182735357

VALIDATION DATA
0.933388888889 0.175476814518
(18000,) (18000,)
16306 31
1168 495

FA FR TA TR 0.00189753320683 0.702345159351 0.297654840649 0.998102466793
0.175476814518  - val loss
0.173505320748  - final_loss
Inside Plateau 1



14  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from 0.17351 to 0.16531, saving model to ./log/cnn/log-weights-0.16531.h5
Epoch 00000: val_loss improved from 0.17351 to 0.16531, saving model to ./log/cnn/log_best_weights.h5
37s - loss: 0.1813 - acc: 0.9357 - val_loss: 0.1653 - val_acc: 0.9402
(180000,) (180000,)
160950 1208
10182 7660

FA FR TA TR 0.0074495245378 0.570675933191 0.429324066809 0.992550475462

VALIDATION DATA
0.940166666667 0.165306499441
(18000,) (18000,)
16260 77
1000 663

FA FR TA TR 0.00471322764277 0.601322910403 0.398677089597 0.995286772357
0.165306499441  - val loss
0.173505320748  - final_loss
Validation Loss decreased. Great work



15  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from 0.16531 to 0.16204, saving model to ./log/cnn/log-weights-0.16204.h5
Epoch 00000: val_loss improved from 0.16531 to 0.16204, saving model to ./log/cnn/log_best_weights.h5
37s - loss: 0.1790 - acc: 0.9370 - val_loss: 0.1620 - val_acc: 0.9410
(180000,) (180000,)
160947 1211
10243 7599

FA FR TA TR 0.00746802501264 0.574094832418 0.425905167582 0.992531974987

VALIDATION DATA
0.941 0.162043788268
(18000,) (18000,)
16255 82
980 683

FA FR TA TR 0.00501928138581 0.589296452195 0.410703547805 0.994980718614
0.162043788268  - val loss
0.165306499441  - final_loss
Validation Loss decreased. Great work



16  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
37s - loss: 0.1764 - acc: 0.9375 - val_loss: 0.1622 - val_acc: 0.9413
(180000,) (180000,)
160584 1574
9539 8303

FA FR TA TR 0.00970658246895 0.534637372492 0.465362627508 0.990293417531

VALIDATION DATA
0.941333333333 0.162171843065
(18000,) (18000,)
16237 100
956 707

FA FR TA TR 0.00612107486075 0.574864702345 0.425135297655 0.993878925139
0.162171843065  - val loss
0.162043788268  - final_loss
Inside Plateau 1



16  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from 0.16204 to 0.15760, saving model to ./log/cnn/log-weights-0.15760.h5
Epoch 00000: val_loss improved from 0.16204 to 0.15760, saving model to ./log/cnn/log_best_weights.h5
37s - loss: 0.1748 - acc: 0.9380 - val_loss: 0.1576 - val_acc: 0.9437
(180000,) (180000,)
160602 1556
9325 8517

FA FR TA TR 0.00959557961988 0.522643201435 0.477356798565 0.99040442038

VALIDATION DATA
0.943666666667 0.157603542864
(18000,) (18000,)
16228 109
905 758

FA FR TA TR 0.00667197159821 0.544197233915 0.455802766085 0.993328028402
0.157603542864  - val loss
0.162043788268  - final_loss
Validation Loss decreased. Great work



17  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
38s - loss: 0.1733 - acc: 0.9384 - val_loss: 0.1594 - val_acc: 0.9389
(180000,) (180000,)
161407 751
11073 6769

FA FR TA TR 0.00463128553633 0.62061428091 0.37938571909 0.995368714464

VALIDATION DATA
0.938944444444 0.159437642962
(18000,) (18000,)
16299 38
1061 602

FA FR TA TR 0.00232600844708 0.638003607937 0.361996392063 0.997673991553
0.159437642962  - val loss
0.157603542864  - final_loss
Inside Plateau 1



17  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from 0.15760 to 0.15319, saving model to ./log/cnn/log-weights-0.15319.h5
Epoch 00000: val_loss improved from 0.15760 to 0.15319, saving model to ./log/cnn/log_best_weights.h5
37s - loss: 0.1714 - acc: 0.9389 - val_loss: 0.1532 - val_acc: 0.9438
(180000,) (180000,)
161048 1110
10134 7708

FA FR TA TR 0.00684517569284 0.567985651833 0.432014348167 0.993154824307

VALIDATION DATA
0.943777777778 0.153191980011
(18000,) (18000,)
16264 73
939 724

FA FR TA TR 0.00446838464834 0.564642212868 0.435357787132 0.995531615352
0.153191980011  - val loss
0.157603542864  - final_loss
Validation Loss decreased. Great work



18  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
38s - loss: 0.1697 - acc: 0.9394 - val_loss: 0.1598 - val_acc: 0.9384
(180000,) (180000,)
161460 698
11061 6781

FA FR TA TR 0.00430444381406 0.619941710571 0.380058289429 0.995695556186

VALIDATION DATA
0.938388888889 0.159762526542
(18000,) (18000,)
16302 35
1074 589

FA FR TA TR 0.00214237620126 0.645820805773 0.354179194227 0.997857623799
0.159762526542  - val loss
0.153191980011  - final_loss
Inside Plateau 1



18  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
38s - loss: 0.1676 - acc: 0.9406 - val_loss: 0.1535 - val_acc: 0.9501
(180000,) (180000,)
159976 2182
7851 9991

FA FR TA TR 0.0134560120376 0.440029144715 0.559970855285 0.986543987962

VALIDATION DATA
0.950111111111 0.153518767668
(18000,) (18000,)
16195 142
756 907

FA FR TA TR 0.00869192630226 0.454600120265 0.545399879735 0.991308073698
0.153518767668  - val loss
0.153191980011  - final_loss
Inside Plateau 2



18  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from 0.15319 to 0.14892, saving model to ./log/cnn/log-weights-0.14892.h5
Epoch 00000: val_loss improved from 0.15319 to 0.14892, saving model to ./log/cnn/log_best_weights.h5
37s - loss: 0.1659 - acc: 0.9405 - val_loss: 0.1489 - val_acc: 0.9446
(180000,) (180000,)
160553 1605
8970 8872

FA FR TA TR 0.00989775404235 0.502746328887 0.497253671113 0.990102245958

VALIDATION DATA
0.944555555556 0.148920081351
(18000,) (18000,)
16234 103
895 768

FA FR TA TR 0.00630470710657 0.538184004811 0.461815995189 0.993695292893
0.148920081351  - val loss
0.153191980011  - final_loss
Validation Loss decreased. Great work



19  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from 0.14892 to 0.14567, saving model to ./log/cnn/log-weights-0.14567.h5
Epoch 00000: val_loss improved from 0.14892 to 0.14567, saving model to ./log/cnn/log_best_weights.h5
37s - loss: 0.1648 - acc: 0.9412 - val_loss: 0.1457 - val_acc: 0.9484
(180000,) (180000,)
160587 1571
8711 9131

FA FR TA TR 0.0096880819941 0.488230019056 0.511769980944 0.990311918006

VALIDATION DATA
0.948444444444 0.145667767008
(18000,) (18000,)
16230 107
821 842

FA FR TA TR 0.006549550101 0.493686109441 0.506313890559 0.993450449899
0.145667767008  - val loss
0.148920081351  - final_loss
Validation Loss decreased. Great work



20  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from 0.14567 to 0.14414, saving model to ./log/cnn/log-weights-0.14414.h5
Epoch 00000: val_loss improved from 0.14567 to 0.14414, saving model to ./log/cnn/log_best_weights.h5
37s - loss: 0.1639 - acc: 0.9412 - val_loss: 0.1441 - val_acc: 0.9473
(180000,) (180000,)
160696 1462
8857 8985

FA FR TA TR 0.00901589807472 0.496412958189 0.503587041811 0.990984101925

VALIDATION DATA
0.947333333333 0.144136907303
(18000,) (18000,)
16255 82
866 797

FA FR TA TR 0.00501928138581 0.520745640409 0.479254359591 0.994980718614
0.144136907303  - val loss
0.145667767008  - final_loss
Validation Loss decreased. Great work



21  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
36s - loss: 0.1621 - acc: 0.9422 - val_loss: 0.1457 - val_acc: 0.9468
(180000,) (180000,)
160730 1428
8857 8985

FA FR TA TR 0.00880622602647 0.496412958189 0.503587041811 0.991193773974

VALIDATION DATA
0.946777777778 0.145691995422
(18000,) (18000,)
16253 84
874 789

FA FR TA TR 0.00514170288303 0.525556223692 0.474443776308 0.994858297117
0.145691995422  - val loss
0.144136907303  - final_loss
Inside Plateau 1



21  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
36s - loss: 0.1614 - acc: 0.9424 - val_loss: 0.1474 - val_acc: 0.9486
(180000,) (180000,)
160137 2021
8022 9820

FA FR TA TR 0.0124631532209 0.449613272055 0.550386727945 0.987536846779

VALIDATION DATA
0.948555555556 0.147377360046
(18000,) (18000,)
16185 152
774 889

FA FR TA TR 0.00930403378833 0.465423932652 0.534576067348 0.990695966212
0.147377360046  - val loss
0.144136907303  - final_loss
Inside Plateau 2



21  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
37s - loss: 0.1606 - acc: 0.9429 - val_loss: 0.1532 - val_acc: 0.9393
(180000,) (180000,)
161434 724
10653 7189

FA FR TA TR 0.00446478126272 0.597074319023 0.402925680977 0.995535218737

VALIDATION DATA
0.939333333333 0.153242178857
(18000,) (18000,)
16300 37
1055 608

FA FR TA TR 0.00226479769848 0.634395670475 0.365604329525 0.997735202302
0.153242178857  - val loss
0.144136907303  - final_loss
Inside Plateau 3



21  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from 0.14414 to 0.13919, saving model to ./log/cnn/log-weights-0.13919.h5
Epoch 00000: val_loss improved from 0.14414 to 0.13919, saving model to ./log/cnn/log_best_weights.h5
37s - loss: 0.1597 - acc: 0.9431 - val_loss: 0.1392 - val_acc: 0.9483
(180000,) (180000,)
160602 1556
8490 9352

FA FR TA TR 0.00959557961988 0.475843515301 0.524156484699 0.99040442038

VALIDATION DATA
0.948277777778 0.139194377906
(18000,) (18000,)
16227 110
821 842

FA FR TA TR 0.00673318234682 0.493686109441 0.506313890559 0.993266817653
0.139194377906  - val loss
0.144136907303  - final_loss
Validation Loss decreased. Great work



22  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
36s - loss: 0.1591 - acc: 0.9434 - val_loss: 0.1403 - val_acc: 0.9489
(180000,) (180000,)
160416 1742
8105 9737

FA FR TA TR 0.0107426090603 0.454265216904 0.545734783096 0.98925739094

VALIDATION DATA
0.948888888889 0.14031414678
(18000,) (18000,)
16217 120
800 863

FA FR TA TR 0.00734528983289 0.481058328322 0.518941671678 0.992654710167
0.14031414678  - val loss
0.139194377906  - final_loss
Inside Plateau 1



22  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
36s - loss: 0.1580 - acc: 0.9436 - val_loss: 0.1434 - val_acc: 0.9429
(180000,) (180000,)
161279 879
9886 7956

FA FR TA TR 0.00542063912974 0.554085864813 0.445914135187 0.99457936087

VALIDATION DATA
0.942888888889 0.143371721115
(18000,) (18000,)
16288 49
979 684

FA FR TA TR 0.00299932668177 0.588695129284 0.411304870716 0.997000673318
0.143371721115  - val loss
0.139194377906  - final_loss
Inside Plateau 2



22  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
36s - loss: 0.1576 - acc: 0.9440 - val_loss: 0.1590 - val_acc: 0.9379
(180000,) (180000,)
161671 487
11523 6319

FA FR TA TR 0.00300324374992 0.645835668647 0.354164331353 0.99699675625

VALIDATION DATA
0.937888888889 0.158977858189
(18000,) (18000,)
16308 29
1089 574

FA FR TA TR 0.00177511170962 0.654840649429 0.345159350571 0.99822488829
0.158977858189  - val loss
0.139194377906  - final_loss
Inside Plateau 3



22  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
37s - loss: 0.1570 - acc: 0.9436 - val_loss: 0.1587 - val_acc: 0.9383
(180000,) (180000,)
161631 527
11254 6588

FA FR TA TR 0.00324991674786 0.630758883533 0.369241116467 0.996750083252

VALIDATION DATA
0.938277777778 0.158719700875
(18000,) (18000,)
16317 20
1091 572

FA FR TA TR 0.00122421497215 0.65604329525 0.34395670475 0.998775785028
0.158719700875  - val loss
0.139194377906  - final_loss
Reducing the learning rate by half



22  iteration
0.005  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from 0.13919 to 0.13626, saving model to ./log/cnn/log-weights-0.13626.h5
Epoch 00000: val_loss improved from 0.13919 to 0.13626, saving model to ./log/cnn/log_best_weights.h5
37s - loss: 0.1520 - acc: 0.9455 - val_loss: 0.1363 - val_acc: 0.9513
(180000,) (180000,)
160280 1878
7797 10045

FA FR TA TR 0.0115812972533 0.437002578186 0.562997421814 0.988418702747

VALIDATION DATA
0.951277777778 0.136259552489
(18000,) (18000,)
16210 127
750 913

FA FR TA TR 0.00777376507315 0.450992182802 0.549007817198 0.992226234927
0.136259552489  - val loss
0.139194377906  - final_loss
Validation Loss decreased. Great work



23  iteration
0.005  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
38s - loss: 0.1517 - acc: 0.9454 - val_loss: 0.1397 - val_acc: 0.9441
(180000,) (180000,)
161296 862
9807 8035

FA FR TA TR 0.00531580310561 0.549658110077 0.450341889923 0.994684196894

VALIDATION DATA
0.944055555556 0.139668934375
(18000,) (18000,)
16285 52
955 708

FA FR TA TR 0.00318295892759 0.574263379435 0.425736620565 0.996817041072
0.139668934375  - val loss
0.136259552489  - final_loss
Inside Plateau 1



23  iteration
0.005  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
37s - loss: 0.1515 - acc: 0.9456 - val_loss: 0.1371 - val_acc: 0.9455
(180000,) (180000,)
160778 1380
8728 9114

FA FR TA TR 0.00851021842894 0.489182827037 0.510817172963 0.991489781571

VALIDATION DATA
0.9455 0.137059632281
(18000,) (18000,)
16236 101
880 783

FA FR TA TR 0.00618228560935 0.529164161155 0.470835838845 0.993817714391
0.137059632281  - val loss
0.136259552489  - final_loss
Inside Plateau 2



23  iteration
0.005  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from 0.13626 to 0.13556, saving model to ./log/cnn/log-weights-0.13556.h5
Epoch 00000: val_loss improved from 0.13626 to 0.13556, saving model to ./log/cnn/log_best_weights.h5
37s - loss: 0.1516 - acc: 0.9455 - val_loss: 0.1356 - val_acc: 0.9511
(180000,) (180000,)
160507 1651
8191 9651

FA FR TA TR 0.01018142799 0.459085304338 0.540914695662 0.98981857201

VALIDATION DATA
0.951111111111 0.135555166069
(18000,) (18000,)
16214 123
757 906

FA FR TA TR 0.00752892207872 0.455201443175 0.544798556825 0.992471077921
0.135555166069  - val loss
0.136259552489  - final_loss
Validation Loss decreased. Great work



24  iteration
0.005  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from 0.13556 to 0.13478, saving model to ./log/cnn/log-weights-0.13478.h5
Epoch 00000: val_loss improved from 0.13556 to 0.13478, saving model to ./log/cnn/log_best_weights.h5
37s - loss: 0.1506 - acc: 0.9459 - val_loss: 0.1348 - val_acc: 0.9470
(180000,) (180000,)
160981 1177
8961 8881

FA FR TA TR 0.00725835296439 0.502241901132 0.497758098868 0.992741647036

VALIDATION DATA
0.947 0.134783595691
(18000,) (18000,)
16258 79
875 788

FA FR TA TR 0.00483564913999 0.526157546603 0.473842453397 0.99516435086
0.134783595691  - val loss
0.135555166069  - final_loss
Validation Loss decreased. Great work



25  iteration
0.005  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from 0.13478 to 0.13450, saving model to ./log/cnn/log-weights-0.13450.h5
Epoch 00000: val_loss improved from 0.13478 to 0.13450, saving model to ./log/cnn/log_best_weights.h5
37s - loss: 0.1511 - acc: 0.9456 - val_loss: 0.1345 - val_acc: 0.9514
(180000,) (180000,)
160311 1847
7818 10024

FA FR TA TR 0.0113901256799 0.438179576281 0.561820423719 0.98860987432

VALIDATION DATA
0.951444444444 0.134496974607
(18000,) (18000,)
16207 130
744 919

FA FR TA TR 0.00795739731897 0.44738424534 0.55261575466 0.992042602681
0.134496974607  - val loss
0.134783595691  - final_loss
Validation Loss decreased. Great work



26  iteration
0.005  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
37s - loss: 0.1505 - acc: 0.9457 - val_loss: 0.1378 - val_acc: 0.9534
(180000,) (180000,)
160160 1998
7492 10350

FA FR TA TR 0.0123213162471 0.419908082054 0.580091917946 0.987678683753

VALIDATION DATA
0.953444444444 0.137790256474
(18000,) (18000,)
16192 145
693 970

FA FR TA TR 0.00887555854808 0.416716776909 0.583283223091 0.991124441452
0.137790256474  - val loss
0.134496974607  - final_loss
Inside Plateau 1



26  iteration
0.005  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from 0.13450 to 0.13335, saving model to ./log/cnn/log-weights-0.13335.h5
Epoch 00000: val_loss improved from 0.13450 to 0.13335, saving model to ./log/cnn/log_best_weights.h5
37s - loss: 0.1501 - acc: 0.9462 - val_loss: 0.1334 - val_acc: 0.9502
(180000,) (180000,)
160646 1512
8331 9511

FA FR TA TR 0.00932423932214 0.466931958301 0.533068041699 0.990675760678

VALIDATION DATA
0.950166666667 0.133353351908
(18000,) (18000,)
16228 109
788 875

FA FR TA TR 0.00667197159821 0.473842453397 0.526157546603 0.993328028402
0.133353351908  - val loss
0.134496974607  - final_loss
Validation Loss decreased. Great work



27  iteration
0.005  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
37s - loss: 0.1500 - acc: 0.9458 - val_loss: 0.1335 - val_acc: 0.9474
(180000,) (180000,)
161032 1126
9003 8839

FA FR TA TR 0.00694384489202 0.504595897321 0.495404102679 0.993056155108

VALIDATION DATA
0.947388888889 0.133525008582
(18000,) (18000,)
16263 74
873 790

FA FR TA TR 0.00452959539695 0.524954900782 0.475045099218 0.995470404603
0.133525008582  - val loss
0.133353351908  - final_loss
Inside Plateau 1



27  iteration
0.005  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from 0.13335 to 0.13324, saving model to ./log/cnn/log-weights-0.13324.h5
Epoch 00000: val_loss improved from 0.13335 to 0.13324, saving model to ./log/cnn/log_best_weights.h5
37s - loss: 0.1500 - acc: 0.9461 - val_loss: 0.1332 - val_acc: 0.9494
(180000,) (180000,)
160691 1467
8349 9493

FA FR TA TR 0.00904673219946 0.46794081381 0.53205918619 0.990953267801

VALIDATION DATA
0.949388888889 0.133242214511
(18000,) (18000,)
16237 100
811 852

FA FR TA TR 0.00612107486075 0.487672880337 0.512327119663 0.993878925139
0.133242214511  - val loss
0.133353351908  - final_loss
Validation Loss decreased. Great work



28  iteration
0.005  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
37s - loss: 0.1495 - acc: 0.9460 - val_loss: 0.1337 - val_acc: 0.9532
(180000,) (180000,)
160243 1915
7613 10229

FA FR TA TR 0.0118094697764 0.426689832978 0.573310167022 0.988190530224

VALIDATION DATA
0.953166666667 0.133725293262
(18000,) (18000,)
16203 134
709 954

FA FR TA TR 0.0082022403134 0.426337943476 0.573662056524 0.991797759687
0.133725293262  - val loss
0.133242214511  - final_loss
Inside Plateau 1



28  iteration
0.005  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from 0.13324 to 0.13158, saving model to ./log/cnn/log-weights-0.13158.h5
Epoch 00000: val_loss improved from 0.13324 to 0.13158, saving model to ./log/cnn/log_best_weights.h5
37s - loss: 0.1493 - acc: 0.9465 - val_loss: 0.1316 - val_acc: 0.9522
(180000,) (180000,)
160482 1676
7990 9852

FA FR TA TR 0.0103355986137 0.447819751149 0.552180248851 0.989664401386

VALIDATION DATA
0.952222222222 0.131582646731
(18000,) (18000,)
16223 114
746 917

FA FR TA TR 0.00697802534125 0.448586891161 0.551413108839 0.993021974659
0.131582646731  - val loss
0.133242214511  - final_loss
Validation Loss decreased. Great work



29  iteration
0.005  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
37s - loss: 0.1496 - acc: 0.9464 - val_loss: 0.1353 - val_acc: 0.9541
(180000,) (180000,)
160126 2032
7424 10418

FA FR TA TR 0.0125309882954 0.416096850129 0.583903149871 0.987469011705

VALIDATION DATA
0.954111111111 0.135273194446
(18000,) (18000,)
16184 153
673 990

FA FR TA TR 0.00936524453694 0.404690318701 0.595309681299 0.990634755463
0.135273194446  - val loss
0.131582646731  - final_loss
Inside Plateau 1



29  iteration
0.005  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
37s - loss: 0.1491 - acc: 0.9468 - val_loss: 0.1359 - val_acc: 0.9552
(180000,) (180000,)
159662 2496
6737 11105

FA FR TA TR 0.0153923950715 0.377592198184 0.622407801816 0.984607604929

VALIDATION DATA
0.955166666667 0.135874860187
(18000,) (18000,)
16176 161
646 1017

FA FR TA TR 0.0098549305258 0.38845460012 0.61154539988 0.990145069474
0.135874860187  - val loss
0.131582646731  - final_loss
Inside Plateau 2



29  iteration
0.005  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from 0.13158 to 0.13144, saving model to ./log/cnn/log-weights-0.13144.h5
Epoch 00000: val_loss improved from 0.13158 to 0.13144, saving model to ./log/cnn/log_best_weights.h5
37s - loss: 0.1491 - acc: 0.9466 - val_loss: 0.1314 - val_acc: 0.9528
(180000,) (180000,)
160280 1878
7606 10236

FA FR TA TR 0.0115812972533 0.42629750028 0.57370249972 0.988418702747

VALIDATION DATA
0.952777777778 0.131441323403
(18000,) (18000,)
16213 124
726 937

FA FR TA TR 0.00759013282732 0.436560432952 0.563439567048 0.992409867173
0.131441323403  - val loss
0.131582646731  - final_loss
Validation Loss decreased. Great work



30  iteration
0.005  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
37s - loss: 0.1487 - acc: 0.9469 - val_loss: 0.1318 - val_acc: 0.9531
(180000,) (180000,)
160205 1953
7475 10367

FA FR TA TR 0.0120438091244 0.418955274072 0.581044725928 0.987956190876

VALIDATION DATA
0.953055555556 0.131761884905
(18000,) (18000,)
16208 129
716 947

FA FR TA TR 0.00789618657036 0.430547203848 0.569452796152 0.99210381343
0.131761884905  - val loss
0.131441323403  - final_loss
Inside Plateau 1



30  iteration
0.005  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
37s - loss: 0.1488 - acc: 0.9468 - val_loss: 0.1342 - val_acc: 0.9543
(180000,) (180000,)
159948 2210
7140 10702

FA FR TA TR 0.0136286831362 0.400179352091 0.599820647909 0.986371316864

VALIDATION DATA
0.954333333333 0.134192418986
(18000,) (18000,)
16175 162
660 1003

FA FR TA TR 0.00991614127441 0.396873120866 0.603126879134 0.990083858726
0.134192418986  - val loss
0.131441323403  - final_loss
Inside Plateau 2



30  iteration
0.005  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
37s - loss: 0.1486 - acc: 0.9467 - val_loss: 0.1334 - val_acc: 0.9463
(180000,) (180000,)
161109 1049
9083 8759

FA FR TA TR 0.00646899937098 0.509079699585 0.490920300415 0.993531000629

VALIDATION DATA
0.946333333333 0.133420432117
(18000,) (18000,)
16266 71
895 768

FA FR TA TR 0.00434596315113 0.538184004811 0.461815995189 0.995654036849
0.133420432117  - val loss
0.131441323403  - final_loss
Inside Plateau 3



30  iteration
0.005  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from 0.13144 to 0.13033, saving model to ./log/cnn/log-weights-0.13033.h5
Epoch 00000: val_loss improved from 0.13144 to 0.13033, saving model to ./log/cnn/log_best_weights.h5
37s - loss: 0.1481 - acc: 0.9470 - val_loss: 0.1303 - val_acc: 0.9517
(180000,) (180000,)
160646 1512
8184 9658

FA FR TA TR 0.00932423932214 0.45869297164 0.54130702836 0.990675760678

VALIDATION DATA
0.951722222222 0.130329513397
(18000,) (18000,)
16232 105
764 899

FA FR TA TR 0.00642712860378 0.459410703548 0.540589296452 0.993572871396
0.130329513397  - val loss
0.131441323403  - final_loss
Validation Loss decreased. Great work



31  iteration
0.005  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
37s - loss: 0.1479 - acc: 0.9470 - val_loss: 0.1506 - val_acc: 0.9567
(180000,) (180000,)
158496 3662
5696 12146

FA FR TA TR 0.0225829129614 0.31924672122 0.68075327878 0.977417087039

VALIDATION DATA
0.956666666667 0.150608486063
(18000,) (18000,)
16064 273
507 1156

FA FR TA TR 0.0167105343698 0.304870715574 0.695129284426 0.98328946563
0.150608486063  - val loss
0.130329513397  - final_loss
Inside Plateau 1



31  iteration
0.005  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from 0.13033 to 0.13027, saving model to ./log/cnn/log-weights-0.13027.h5
Epoch 00000: val_loss improved from 0.13033 to 0.13027, saving model to ./log/cnn/log_best_weights.h5
37s - loss: 0.1483 - acc: 0.9469 - val_loss: 0.1303 - val_acc: 0.9533
(180000,) (180000,)
160294 1864
7596 10246

FA FR TA TR 0.011494961704 0.425737024997 0.574262975003 0.988505038296

VALIDATION DATA
0.953277777778 0.130273826801
(18000,) (18000,)
16217 120
721 942

FA FR TA TR 0.00734528983289 0.4335538184 0.5664461816 0.992654710167
0.130273826801  - val loss
0.130329513397  - final_loss
Validation Loss decreased. Great work



32  iteration
0.005  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from 0.13027 to 0.12993, saving model to ./log/cnn/log-weights-0.12993.h5
Epoch 00000: val_loss improved from 0.13027 to 0.12993, saving model to ./log/cnn/log_best_weights.h5
37s - loss: 0.1477 - acc: 0.9472 - val_loss: 0.1299 - val_acc: 0.9504
(180000,) (180000,)
160513 1645
7876 9966

FA FR TA TR 0.0101444270403 0.441430332922 0.558569667078 0.98985557296

VALIDATION DATA
0.950444444444 0.12993171317
(18000,) (18000,)
16234 103
789 874

FA FR TA TR 0.00630470710657 0.474443776308 0.525556223692 0.993695292893
0.12993171317  - val loss
0.130273826801  - final_loss
Validation Loss decreased. Great work



33  iteration
0.005  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from 0.12993 to 0.12974, saving model to ./log/cnn/log-weights-0.12974.h5
Epoch 00000: val_loss improved from 0.12993 to 0.12974, saving model to ./log/cnn/log_best_weights.h5
37s - loss: 0.1476 - acc: 0.9470 - val_loss: 0.1297 - val_acc: 0.9506
(180000,) (180000,)
160597 1561
7996 9846

FA FR TA TR 0.00962641374462 0.448156036319 0.551843963681 0.990373586255

VALIDATION DATA
0.950555555556 0.129742696769
(18000,) (18000,)
16236 101
789 874

FA FR TA TR 0.00618228560935 0.474443776308 0.525556223692 0.993817714391
0.129742696769  - val loss
0.12993171317  - final_loss
Validation Loss decreased. Great work



34  iteration
0.005  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from 0.12974 to 0.12951, saving model to ./log/cnn/log-weights-0.12951.h5
Epoch 00000: val_loss improved from 0.12974 to 0.12951, saving model to ./log/cnn/log_best_weights.h5
38s - loss: 0.1475 - acc: 0.9471 - val_loss: 0.1295 - val_acc: 0.9533
(180000,) (180000,)
160232 1926
7414 10428

FA FR TA TR 0.0118773048508 0.415536374846 0.584463625154 0.988122695149

VALIDATION DATA
0.953333333333 0.129509168595
(18000,) (18000,)
16213 124
716 947

FA FR TA TR 0.00759013282732 0.430547203848 0.569452796152 0.992409867173
0.129509168595  - val loss
0.129742696769  - final_loss
Validation Loss decreased. Great work



35  iteration
0.005  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
37s - loss: 0.1476 - acc: 0.9472 - val_loss: 0.1334 - val_acc: 0.9551
(180000,) (180000,)
159821 2337
6925 10917

FA FR TA TR 0.0144118699047 0.388129133505 0.611870866495 0.985588130095

VALIDATION DATA
0.955055555556 0.133377804392
(18000,) (18000,)
16171 166
643 1020

FA FR TA TR 0.0101609842688 0.386650631389 0.613349368611 0.989839015731
0.133377804392  - val loss
0.129509168595  - final_loss
Inside Plateau 1



35  iteration
0.005  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
37s - loss: 0.1478 - acc: 0.9471 - val_loss: 0.1299 - val_acc: 0.9528
(180000,) (180000,)
160300 1858
7566 10276

FA FR TA TR 0.0114579607543 0.424055599148 0.575944400852 0.988542039246

VALIDATION DATA
0.952833333333 0.129925216118
(18000,) (18000,)
16209 128
721 942

FA FR TA TR 0.00783497582175 0.4335538184 0.5664461816 0.992165024178
0.129925216118  - val loss
0.129509168595  - final_loss
Inside Plateau 2



35  iteration
0.005  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
37s - loss: 0.1469 - acc: 0.9472 - val_loss: 0.1314 - val_acc: 0.9544
(180000,) (180000,)
160014 2144
7098 10744

FA FR TA TR 0.0132216726896 0.397825355902 0.602174644098 0.98677832731

VALIDATION DATA
0.954444444444 0.131350676649
(18000,) (18000,)
16194 143
677 986

FA FR TA TR 0.00875313705087 0.407095610343 0.592904389657 0.991246862949
0.131350676649  - val loss
0.129509168595  - final_loss
Inside Plateau 3



35  iteration
0.005  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
37s - loss: 0.1471 - acc: 0.9476 - val_loss: 0.1411 - val_acc: 0.9563
(180000,) (180000,)
159364 2794
6418 11424

FA FR TA TR 0.0172301089061 0.359713036655 0.640286963345 0.982769891094

VALIDATION DATA
0.956277777778 0.14108367308
(18000,) (18000,)
16123 214
573 1090

FA FR TA TR 0.013099100202 0.344558027661 0.655441972339 0.986900899798
0.14108367308  - val loss
0.129509168595  - final_loss
Reducing the learning rate by half



35  iteration
0.0025  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from 0.12951 to 0.12865, saving model to ./log/cnn/log-weights-0.12865.h5
Epoch 00000: val_loss improved from 0.12951 to 0.12865, saving model to ./log/cnn/log_best_weights.h5
38s - loss: 0.1451 - acc: 0.9480 - val_loss: 0.1287 - val_acc: 0.9492
(180000,) (180000,)
160813 1345
8356 9486

FA FR TA TR 0.00829437955574 0.468333146508 0.531666853492 0.991705620444

VALIDATION DATA
0.949222222222 0.128651685778
(18000,) (18000,)
16245 92
822 841

FA FR TA TR 0.00563138887189 0.494287432351 0.505712567649 0.994368611128
0.128651685778  - val loss
0.129509168595  - final_loss
Validation Loss decreased. Great work



36  iteration
0.0025  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
37s - loss: 0.1448 - acc: 0.9480 - val_loss: 0.1319 - val_acc: 0.9553
(180000,) (180000,)
160014 2144
7155 10687

FA FR TA TR 0.0132216726896 0.401020065015 0.598979934985 0.98677832731

VALIDATION DATA
0.955277777778 0.131924946603
(18000,) (18000,)
16174 163
642 1021

FA FR TA TR 0.00997735202302 0.386049308479 0.613950691521 0.990022647977
0.131924946603  - val loss
0.128651685778  - final_loss
Inside Plateau 1



36  iteration
0.0025  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
37s - loss: 0.1451 - acc: 0.9478 - val_loss: 0.1289 - val_acc: 0.9511
(180000,) (180000,)
160669 1489
8108 9734

FA FR TA TR 0.00918240234833 0.454433359489 0.545566640511 0.990817597652

VALIDATION DATA
0.951055555556 0.128941202717
(18000,) (18000,)
16233 104
777 886

FA FR TA TR 0.00636591785518 0.467227901383 0.532772098617 0.993634082145
0.128941202717  - val loss
0.128651685778  - final_loss
Inside Plateau 2



36  iteration
0.0025  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from 0.12865 to 0.12856, saving model to ./log/cnn/log-weights-0.12856.h5
Epoch 00000: val_loss improved from 0.12865 to 0.12856, saving model to ./log/cnn/log_best_weights.h5
38s - loss: 0.1449 - acc: 0.9480 - val_loss: 0.1286 - val_acc: 0.9498
(180000,) (180000,)
160859 1299
8473 9369

FA FR TA TR 0.00801070560811 0.47489070732 0.52510929268 0.991989294392

VALIDATION DATA
0.949833333333 0.128561993857
(18000,) (18000,)
16244 93
810 853

FA FR TA TR 0.00569259962049 0.487071557426 0.512928442574 0.99430740038
0.128561993857  - val loss
0.128651685778  - final_loss
Validation Loss decreased. Great work



37  iteration
0.0025  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from 0.12856 to 0.12826, saving model to ./log/cnn/log-weights-0.12826.h5
Epoch 00000: val_loss improved from 0.12856 to 0.12826, saving model to ./log/cnn/log_best_weights.h5
38s - loss: 0.1449 - acc: 0.9480 - val_loss: 0.1283 - val_acc: 0.9530
(180000,) (180000,)
160430 1728
7712 10130

FA FR TA TR 0.010656273511 0.43223853828 0.56776146172 0.989343726489

VALIDATION DATA
0.953 0.128260935671
(18000,) (18000,)
16219 118
728 935

FA FR TA TR 0.00722286833568 0.437763078773 0.562236921227 0.992777131664
0.128260935671  - val loss
0.128561993857  - final_loss
Validation Loss decreased. Great work



38  iteration
0.0025  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
37s - loss: 0.1449 - acc: 0.9481 - val_loss: 0.1390 - val_acc: 0.9571
(180000,) (180000,)
159286 2872
6282 11560

FA FR TA TR 0.0177111212521 0.352090572806 0.647909427194 0.982288878748

VALIDATION DATA
0.957055555556 0.139028339353
(18000,) (18000,)
16125 212
561 1102

FA FR TA TR 0.0129766787048 0.337342152736 0.662657847264 0.987023321295
0.139028339353  - val loss
0.128260935671  - final_loss
Inside Plateau 1



38  iteration
0.0025  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from 0.12826 to 0.12809, saving model to ./log/cnn/log-weights-0.12809.h5
Epoch 00000: val_loss improved from 0.12826 to 0.12809, saving model to ./log/cnn/log_best_weights.h5
38s - loss: 0.1446 - acc: 0.9480 - val_loss: 0.1281 - val_acc: 0.9534
(180000,) (180000,)
160443 1715
7695 10147

FA FR TA TR 0.0105761047867 0.431285730299 0.568714269701 0.989423895213

VALIDATION DATA
0.953388888889 0.12808531236
(18000,) (18000,)
16226 111
728 935

FA FR TA TR 0.00679439309543 0.437763078773 0.562236921227 0.993205606905
0.12808531236  - val loss
0.128260935671  - final_loss
Validation Loss decreased. Great work



39  iteration
0.0025  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
38s - loss: 0.1446 - acc: 0.9480 - val_loss: 0.1290 - val_acc: 0.9543
(180000,) (180000,)
160106 2052
7175 10667

FA FR TA TR 0.0126543247943 0.402141015581 0.597858984419 0.987345675206

VALIDATION DATA
0.954277777778 0.12902053827
(18000,) (18000,)
16199 138
685 978

FA FR TA TR 0.00844708330783 0.411906193626 0.588093806374 0.991552916692
0.12902053827  - val loss
0.12808531236  - final_loss
Inside Plateau 1



39  iteration
0.0025  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from 0.12809 to 0.12769, saving model to ./log/cnn/log-weights-0.12769.h5
Epoch 00000: val_loss improved from 0.12809 to 0.12769, saving model to ./log/cnn/log_best_weights.h5
38s - loss: 0.1446 - acc: 0.9478 - val_loss: 0.1277 - val_acc: 0.9533
(180000,) (180000,)
160271 1887
7395 10447

FA FR TA TR 0.0116367986778 0.414471471808 0.585528528192 0.988363201322

VALIDATION DATA
0.953333333333 0.127690521889
(18000,) (18000,)
16214 123
717 946

FA FR TA TR 0.00752892207872 0.431148526759 0.568851473241 0.992471077921
0.127690521889  - val loss
0.12808531236  - final_loss
Validation Loss decreased. Great work



40  iteration
0.0025  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
37s - loss: 0.1443 - acc: 0.9481 - val_loss: 0.1281 - val_acc: 0.9522
(180000,) (180000,)
160545 1613
7824 10018

FA FR TA TR 0.00994708864194 0.438515861451 0.561484138549 0.990052911358

VALIDATION DATA
0.952222222222 0.128090653254
(18000,) (18000,)
16231 106
754 909

FA FR TA TR 0.00648833935239 0.453397474444 0.546602525556 0.993511660648
0.128090653254  - val loss
0.127690521889  - final_loss
Inside Plateau 1



40  iteration
0.0025  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
38s - loss: 0.1447 - acc: 0.9481 - val_loss: 0.1314 - val_acc: 0.9553
(180000,) (180000,)
159705 2453
6687 11155

FA FR TA TR 0.0151272215987 0.374789821769 0.625210178231 0.984872778401

VALIDATION DATA
0.955333333333 0.131423942082
(18000,) (18000,)
16167 170
634 1029

FA FR TA TR 0.0104058272633 0.381238725195 0.618761274805 0.989594172737
0.131423942082  - val loss
0.127690521889  - final_loss
Inside Plateau 2



40  iteration
0.0025  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
38s - loss: 0.1448 - acc: 0.9481 - val_loss: 0.1290 - val_acc: 0.9488
(180000,) (180000,)
160975 1183
8642 9200

FA FR TA TR 0.00729535391408 0.484362739603 0.515637260397 0.992704646086

VALIDATION DATA
0.948777777778 0.128961471836
(18000,) (18000,)
16253 84
838 825

FA FR TA TR 0.00514170288303 0.503908598918 0.496091401082 0.994858297117
0.128961471836  - val loss
0.127690521889  - final_loss
Inside Plateau 3



40  iteration
0.0025  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
38s - loss: 0.1443 - acc: 0.9482 - val_loss: 0.1296 - val_acc: 0.9554
(180000,) (180000,)
160030 2128
7062 10780

FA FR TA TR 0.0131230034904 0.395807644883 0.604192355117 0.98687699651

VALIDATION DATA
0.955444444444 0.129632669078
(18000,) (18000,)
16190 147
655 1008

FA FR TA TR 0.0089979800453 0.393866506314 0.606133493686 0.991002019955
0.129632669078  - val loss
0.127690521889  - final_loss
Reducing the learning rate by half



40  iteration
0.00125  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
38s - loss: 0.1433 - acc: 0.9485 - val_loss: 0.1292 - val_acc: 0.9552
(180000,) (180000,)
160049 2109
7105 10737

FA FR TA TR 0.0130058338164 0.3982176886 0.6017823114 0.986994166184

VALIDATION DATA
0.955222222222 0.12916840648
(18000,) (18000,)
16189 148
658 1005

FA FR TA TR 0.0090591907939 0.395670475045 0.604329524955 0.990940809206
0.12916840648  - val loss
0.127690521889  - final_loss
Inside Plateau 1



40  iteration
0.00125  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
38s - loss: 0.1435 - acc: 0.9485 - val_loss: 0.1293 - val_acc: 0.9556
(180000,) (180000,)
159964 2194
7000 10842

FA FR TA TR 0.013530013937 0.392332698128 0.607667301872 0.986469986063

VALIDATION DATA
0.955611111111 0.129326239159
(18000,) (18000,)
16186 151
648 1015

FA FR TA TR 0.00924282303973 0.389657245941 0.610342754059 0.99075717696
0.129326239159  - val loss
0.127690521889  - final_loss
Inside Plateau 2



40  iteration
0.00125  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from 0.12769 to 0.12704, saving model to ./log/cnn/log-weights-0.12704.h5
Epoch 00000: val_loss improved from 0.12769 to 0.12704, saving model to ./log/cnn/log_best_weights.h5
38s - loss: 0.1436 - acc: 0.9484 - val_loss: 0.1270 - val_acc: 0.9533
(180000,) (180000,)
160401 1757
7545 10297

FA FR TA TR 0.0108351114345 0.422878601054 0.577121398946 0.989164888565

VALIDATION DATA
0.953333333333 0.127038308766
(18000,) (18000,)
16225 112
728 935

FA FR TA TR 0.00685560384404 0.437763078773 0.562236921227 0.993144396156
0.127038308766  - val loss
0.127690521889  - final_loss
Validation Loss decreased. Great work



41  iteration
0.00125  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
38s - loss: 0.1435 - acc: 0.9483 - val_loss: 0.1293 - val_acc: 0.9559
(180000,) (180000,)
160013 2145
7065 10777

FA FR TA TR 0.0132278395145 0.395975787468 0.604024212532 0.986772160485

VALIDATION DATA
0.955888888889 0.12930871941
(18000,) (18000,)
16188 149
645 1018

FA FR TA TR 0.00912040154251 0.38785327721 0.61214672279 0.990879598457
0.12930871941  - val loss
0.127038308766  - final_loss
Inside Plateau 1



41  iteration
0.00125  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
38s - loss: 0.1437 - acc: 0.9486 - val_loss: 0.1281 - val_acc: 0.9548
(180000,) (180000,)
160121 2037
7167 10675

FA FR TA TR 0.0125618224201 0.401692635355 0.598307364645 0.98743817758

VALIDATION DATA
0.954777777778 0.128051589512
(18000,) (18000,)
16197 140
674 989

FA FR TA TR 0.00856950480504 0.405291641612 0.594708358388 0.991430495195
0.128051589512  - val loss
0.127038308766  - final_loss
Inside Plateau 2



41  iteration
0.00125  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from 0.12704 to 0.12662, saving model to ./log/cnn/log-weights-0.12662.h5
Epoch 00000: val_loss improved from 0.12704 to 0.12662, saving model to ./log/cnn/log_best_weights.h5
38s - loss: 0.1429 - acc: 0.9486 - val_loss: 0.1266 - val_acc: 0.9524
(180000,) (180000,)
160522 1636
7769 10073

FA FR TA TR 0.0100889256158 0.435433247394 0.564566752606 0.989911074384

VALIDATION DATA
0.952444444444 0.1266184821
(18000,) (18000,)
16233 104
752 911

FA FR TA TR 0.00636591785518 0.452194828623 0.547805171377 0.993634082145
0.1266184821  - val loss
0.127038308766  - final_loss
Validation Loss decreased. Great work



42  iteration
0.00125  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
38s - loss: 0.1436 - acc: 0.9485 - val_loss: 0.1287 - val_acc: 0.9555
(180000,) (180000,)
160139 2019
7248 10594

FA FR TA TR 0.012450819571 0.406232485147 0.593767514853 0.987549180429

VALIDATION DATA
0.9555 0.128681428896
(18000,) (18000,)
16192 145
656 1007

FA FR TA TR 0.00887555854808 0.394467829224 0.605532170776 0.991124441452
0.128681428896  - val loss
0.1266184821  - final_loss
Inside Plateau 1



42  iteration
0.00125  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from 0.12662 to 0.12658, saving model to ./log/cnn/log-weights-0.12658.h5
Epoch 00000: val_loss improved from 0.12662 to 0.12658, saving model to ./log/cnn/log_best_weights.h5
38s - loss: 0.1430 - acc: 0.9485 - val_loss: 0.1266 - val_acc: 0.9524
(180000,) (180000,)
160477 1681
7702 10140

FA FR TA TR 0.0103664327384 0.431678062997 0.568321937003 0.989633567262

VALIDATION DATA
0.952444444444 0.126575616108
(18000,) (18000,)
16223 114
742 921

FA FR TA TR 0.00697802534125 0.446181599519 0.553818400481 0.993021974659
0.126575616108  - val loss
0.1266184821  - final_loss
Validation Loss decreased. Great work



43  iteration
0.00125  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
38s - loss: 0.1435 - acc: 0.9487 - val_loss: 0.1266 - val_acc: 0.9523
(180000,) (180000,)
160509 1649
7751 10091

FA FR TA TR 0.0101690943401 0.434424391884 0.565575608116 0.98983090566

VALIDATION DATA
0.952277777778 0.126588109841
(18000,) (18000,)
16228 109
750 913

FA FR TA TR 0.00667197159821 0.450992182802 0.549007817198 0.993328028402
0.126588109841  - val loss
0.126575616108  - final_loss
Inside Plateau 1



43  iteration
0.00125  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
38s - loss: 0.1434 - acc: 0.9483 - val_loss: 0.1298 - val_acc: 0.9561
(180000,) (180000,)
159852 2306
6828 11014

FA FR TA TR 0.0142206983313 0.38269252326 0.61730747674 0.985779301669

VALIDATION DATA
0.956111111111 0.12984504054
(18000,) (18000,)
16179 158
632 1031

FA FR TA TR 0.00967129827998 0.380036079375 0.619963920625 0.99032870172
0.12984504054  - val loss
0.126575616108  - final_loss
Inside Plateau 2



43  iteration
0.00125  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
38s - loss: 0.1430 - acc: 0.9486 - val_loss: 0.1270 - val_acc: 0.9501
(180000,) (180000,)
160808 1350
8305 9537

FA FR TA TR 0.00832521368048 0.465474722565 0.534525277435 0.99167478632

VALIDATION DATA
0.950055555556 0.127011253903
(18000,) (18000,)
16246 91
808 855

FA FR TA TR 0.00557017812328 0.485868911606 0.514131088394 0.994429821877
0.127011253903  - val loss
0.126575616108  - final_loss
Inside Plateau 3



43  iteration
0.00125  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
38s - loss: 0.1432 - acc: 0.9485 - val_loss: 0.1267 - val_acc: 0.9509
(180000,) (180000,)
160665 1493
7995 9847

FA FR TA TR 0.00920706964812 0.44809998879 0.55190001121 0.990792930352

VALIDATION DATA
0.950888888889 0.12669087357
(18000,) (18000,)
16239 98
786 877

FA FR TA TR 0.00599865336353 0.472639807577 0.527360192423 0.994001346636
0.12669087357  - val loss
0.126575616108  - final_loss
Reducing the learning rate by half



43  iteration
0.000625  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
38s - loss: 0.1430 - acc: 0.9486 - val_loss: 0.1288 - val_acc: 0.9554
(180000,) (180000,)
160079 2079
7161 10681

FA FR TA TR 0.0128208290679 0.401356350185 0.598643649815 0.987179170932

VALIDATION DATA
0.955444444444 0.128831872072
(18000,) (18000,)
16187 150
652 1011

FA FR TA TR 0.00918161229112 0.392062537583 0.607937462417 0.990818387709
0.128831872072  - val loss
0.126575616108  - final_loss
Inside Plateau 1



43  iteration
0.000625  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
38s - loss: 0.1426 - acc: 0.9486 - val_loss: 0.1280 - val_acc: 0.9552
(180000,) (180000,)
160106 2052
7147 10695

FA FR TA TR 0.0126543247943 0.400571684789 0.599428315211 0.987345675206

VALIDATION DATA
0.955166666667 0.128017967042
(18000,) (18000,)
16193 144
663 1000

FA FR TA TR 0.00881434779947 0.398677089597 0.601322910403 0.991185652201
0.128017967042  - val loss
0.126575616108  - final_loss
Inside Plateau 2



43  iteration
0.000625  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from 0.12658 to 0.12631, saving model to ./log/cnn/log-weights-0.12631.h5
Epoch 00000: val_loss improved from 0.12658 to 0.12631, saving model to ./log/cnn/log_best_weights.h5
38s - loss: 0.1425 - acc: 0.9488 - val_loss: 0.1263 - val_acc: 0.9531
(180000,) (180000,)
160428 1730
7593 10249

FA FR TA TR 0.0106686071609 0.425568882412 0.574431117588 0.989331392839

VALIDATION DATA
0.953111111111 0.126305604617
(18000,) (18000,)
16220 117
727 936

FA FR TA TR 0.00716165758707 0.437161755863 0.562838244137 0.992838342413
0.126305604617  - val loss
0.126575616108  - final_loss
Validation Loss decreased. Great work



44  iteration
0.000625  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
38s - loss: 0.1426 - acc: 0.9489 - val_loss: 0.1284 - val_acc: 0.9554
(180000,) (180000,)
160043 2115
7068 10774

FA FR TA TR 0.0130428347661 0.396143930053 0.603856069947 0.986957165234

VALIDATION DATA
0.955444444444 0.128406166034
(18000,) (18000,)
16188 149
653 1010

FA FR TA TR 0.00912040154251 0.392663860493 0.607336139507 0.990879598457
0.128406166034  - val loss
0.126305604617  - final_loss
Inside Plateau 1



44  iteration
0.000625  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from 0.12631 to 0.12611, saving model to ./log/cnn/log-weights-0.12611.h5
Epoch 00000: val_loss improved from 0.12631 to 0.12611, saving model to ./log/cnn/log_best_weights.h5
38s - loss: 0.1428 - acc: 0.9486 - val_loss: 0.1261 - val_acc: 0.9528
(180000,) (180000,)
160494 1664
7698 10144

FA FR TA TR 0.0102615967143 0.431453872884 0.568546127116 0.989738403286

VALIDATION DATA
0.952833333333 0.126108691987
(18000,) (18000,)
16228 109
740 923

FA FR TA TR 0.00667197159821 0.444978953698 0.555021046302 0.993328028402
0.126108691987  - val loss
0.126305604617  - final_loss
Validation Loss decreased. Great work



45  iteration
0.000625  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
38s - loss: 0.1426 - acc: 0.9486 - val_loss: 0.1271 - val_acc: 0.9550
(180000,) (180000,)
160170 1988
7201 10641

FA FR TA TR 0.0122596479976 0.403598251317 0.596401748683 0.987740352002

VALIDATION DATA
0.955 0.12710285402
(18000,) (18000,)
16202 135
675 988

FA FR TA TR 0.00826345106201 0.405892964522 0.594107035478 0.991736548938
0.12710285402  - val loss
0.126108691987  - final_loss
Inside Plateau 1



45  iteration
0.000625  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
38s - loss: 0.1429 - acc: 0.9483 - val_loss: 0.1310 - val_acc: 0.9566
(180000,) (180000,)
159757 2401
6688 11154

FA FR TA TR 0.0148065467014 0.374845869297 0.625154130703 0.985193453299

VALIDATION DATA
0.956611111111 0.130978517774
(18000,) (18000,)
16167 170
611 1052

FA FR TA TR 0.0104058272633 0.367408298256 0.632591701744 0.989594172737
0.130978517774  - val loss
0.126108691987  - final_loss
Inside Plateau 2



45  iteration
0.000625  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
38s - loss: 0.1425 - acc: 0.9485 - val_loss: 0.1267 - val_acc: 0.9548
(180000,) (180000,)
160230 1928
7288 10554

FA FR TA TR 0.0118896385007 0.40847438628 0.59152561372 0.988110361499

VALIDATION DATA
0.954777777778 0.126709995376
(18000,) (18000,)
16208 129
685 978

FA FR TA TR 0.00789618657036 0.411906193626 0.588093806374 0.99210381343
0.126709995376  - val loss
0.126108691987  - final_loss
Inside Plateau 3



45  iteration
0.000625  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
38s - loss: 0.1427 - acc: 0.9487 - val_loss: 0.1281 - val_acc: 0.9554
(180000,) (180000,)
160066 2092
7076 10766

FA FR TA TR 0.0129009977923 0.396592310279 0.603407689721 0.987099002208

VALIDATION DATA
0.955444444444 0.128120545086
(18000,) (18000,)
16191 146
656 1007

FA FR TA TR 0.00893676929669 0.394467829224 0.605532170776 0.991063230703
0.128120545086  - val loss
0.126108691987  - final_loss
Reducing the learning rate by half



45  iteration
0.0003125  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from 0.12611 to 0.12606, saving model to ./log/cnn/log-weights-0.12606.h5
Epoch 00000: val_loss improved from 0.12611 to 0.12606, saving model to ./log/cnn/log_best_weights.h5
39s - loss: 0.1424 - acc: 0.9487 - val_loss: 0.1261 - val_acc: 0.9529
(180000,) (180000,)
160514 1644
7739 10103

FA FR TA TR 0.0101382602153 0.433751821545 0.566248178455 0.989861739785

VALIDATION DATA
0.952888888889 0.126061707606
(18000,) (18000,)
16230 107
741 922

FA FR TA TR 0.006549550101 0.445580276609 0.554419723391 0.993450449899
0.126061707606  - val loss
0.126108691987  - final_loss
Validation Loss decreased. Great work



46  iteration
0.0003125  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
38s - loss: 0.1423 - acc: 0.9487 - val_loss: 0.1266 - val_acc: 0.9548
(180000,) (180000,)
160250 1908
7324 10518

FA FR TA TR 0.0117663020018 0.410492097299 0.589507902701 0.988233697998

VALIDATION DATA
0.954777777778 0.126617016024
(18000,) (18000,)
16209 128
686 977

FA FR TA TR 0.00783497582175 0.412507516536 0.587492483464 0.992165024178
0.126617016024  - val loss
0.126061707606  - final_loss
Inside Plateau 1



46  iteration
0.0003125  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
38s - loss: 0.1424 - acc: 0.9487 - val_loss: 0.1263 - val_acc: 0.9542
(180000,) (180000,)
160369 1789
7484 10358

FA FR TA TR 0.0110324498329 0.419459701827 0.580540298173 0.988967550167

VALIDATION DATA
0.954166666667 0.126320992106
(18000,) (18000,)
16216 121
704 959

FA FR TA TR 0.0074065005815 0.423331328924 0.576668671076 0.992593499418
0.126320992106  - val loss
0.126061707606  - final_loss
Inside Plateau 2



46  iteration
0.0003125  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
38s - loss: 0.1425 - acc: 0.9488 - val_loss: 0.1280 - val_acc: 0.9554
(180000,) (180000,)
160064 2094
7084 10758

FA FR TA TR 0.0129133314422 0.397040690506 0.602959309494 0.987086668558

VALIDATION DATA
0.955388888889 0.128041292522
(18000,) (18000,)
16189 148
655 1008

FA FR TA TR 0.0090591907939 0.393866506314 0.606133493686 0.990940809206
0.128041292522  - val loss
0.126061707606  - final_loss
Inside Plateau 3



46  iteration
0.0003125  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
38s - loss: 0.1423 - acc: 0.9489 - val_loss: 0.1285 - val_acc: 0.9557
(180000,) (180000,)
159966 2192
6956 10886

FA FR TA TR 0.0135176802871 0.389866606883 0.610133393117 0.986482319713

VALIDATION DATA
0.955722222222 0.128540515761
(18000,) (18000,)
16186 151
646 1017

FA FR TA TR 0.00924282303973 0.38845460012 0.61154539988 0.99075717696
0.128540515761  - val loss
0.126061707606  - final_loss
Reducing the learning rate by half



46  iteration
0.00015625  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
38s - loss: 0.1422 - acc: 0.9488 - val_loss: 0.1263 - val_acc: 0.9539
(180000,) (180000,)
160372 1786
7482 10360

FA FR TA TR 0.011013949358 0.419347606771 0.580652393229 0.988986050642

VALIDATION DATA
0.953888888889 0.126306216882
(18000,) (18000,)
16214 123
707 956

FA FR TA TR 0.00752892207872 0.425135297655 0.574864702345 0.992471077921
0.126306216882  - val loss
0.126061707606  - final_loss
Inside Plateau 1



46  iteration
0.00015625  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
38s - loss: 0.1418 - acc: 0.9487 - val_loss: 0.1288 - val_acc: 0.9561
(180000,) (180000,)
159923 2235
6918 10924

FA FR TA TR 0.0137828537599 0.387736800807 0.612263199193 0.98621714624

VALIDATION DATA
0.956055555556 0.128797932804
(18000,) (18000,)
16184 153
638 1025

FA FR TA TR 0.00936524453694 0.383644016837 0.616355983163 0.990634755463
0.128797932804  - val loss
0.126061707606  - final_loss
Inside Plateau 2



46  iteration
0.00015625  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
38s - loss: 0.1424 - acc: 0.9484 - val_loss: 0.1291 - val_acc: 0.9562
(180000,) (180000,)
159909 2249
6881 10961

FA FR TA TR 0.0138691893092 0.38566304226 0.61433695774 0.986130810691

VALIDATION DATA
0.956222222222 0.129127979991
(18000,) (18000,)
16182 155
633 1030

FA FR TA TR 0.00948766603416 0.380637402285 0.619362597715 0.990512333966
0.129127979991  - val loss
0.126061707606  - final_loss
Inside Plateau 3



46  iteration
0.00015625  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
38s - loss: 0.1421 - acc: 0.9489 - val_loss: 0.1276 - val_acc: 0.9553
(180000,) (180000,)
160093 2065
7113 10729

FA FR TA TR 0.0127344935187 0.398666068826 0.601333931174 0.987265506481

VALIDATION DATA
0.955277777778 0.127619712803
(18000,) (18000,)
16195 142
663 1000

FA FR TA TR 0.00869192630226 0.398677089597 0.601322910403 0.991308073698
0.127619712803  - val loss
0.126061707606  - final_loss
Reducing the learning rate by half



46  iteration
7.8125e-05  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
38s - loss: 0.1424 - acc: 0.9487 - val_loss: 0.1279 - val_acc: 0.9553
(180000,) (180000,)
160070 2088
7077 10765

FA FR TA TR 0.0128763304925 0.396648357807 0.603351642193 0.987123669508

VALIDATION DATA
0.955333333333 0.127850197948
(18000,) (18000,)
16191 146
658 1005

FA FR TA TR 0.00893676929669 0.395670475045 0.604329524955 0.991063230703
0.127850197948  - val loss
0.126061707606  - final_loss
Inside Plateau 1



46  iteration
7.8125e-05  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
39s - loss: 0.1422 - acc: 0.9488 - val_loss: 0.1271 - val_acc: 0.9551
(180000,) (180000,)
160176 1982
7209 10633

FA FR TA TR 0.0122226470479 0.404046631544 0.595953368456 0.987777352952

VALIDATION DATA
0.955055555556 0.127068273604
(18000,) (18000,)
16200 137
672 991

FA FR TA TR 0.00838587255922 0.404088995791 0.595911004209 0.991614127441
0.127068273604  - val loss
0.126061707606  - final_loss
Inside Plateau 2



46  iteration
7.8125e-05  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
39s - loss: 0.1424 - acc: 0.9487 - val_loss: 0.1270 - val_acc: 0.9550
(180000,) (180000,)
160201 1957
7239 10603

FA FR TA TR 0.0120684764242 0.405728057393 0.594271942607 0.987931523576

VALIDATION DATA
0.955 0.126984142476
(18000,) (18000,)
16202 135
675 988

FA FR TA TR 0.00826345106201 0.405892964522 0.594107035478 0.991736548938
0.126984142476  - val loss
0.126061707606  - final_loss
Inside Plateau 3



46  iteration
7.8125e-05  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
39s - loss: 0.1418 - acc: 0.9489 - val_loss: 0.1269 - val_acc: 0.9551
(180000,) (180000,)
160215 1943
7256 10586

FA FR TA TR 0.0119821408749 0.406680865374 0.593319134626 0.988017859125

VALIDATION DATA
0.955055555556 0.126855765286
(18000,) (18000,)
16205 132
677 986

FA FR TA TR 0.00807981881618 0.407095610343 0.592904389657 0.991920181184
0.126855765286  - val loss
0.126061707606  - final_loss
Reducing the learning rate by half



46  iteration
3.90625e-05  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
39s - loss: 0.1422 - acc: 0.9486 - val_loss: 0.1267 - val_acc: 0.9551
(180000,) (180000,)
160238 1920
7299 10543

FA FR TA TR 0.0118403039011 0.409090909091 0.590909090909 0.988159696099

VALIDATION DATA
0.955055555556 0.126725726459
(18000,) (18000,)
16207 130
679 984

FA FR TA TR 0.00795739731897 0.408298256164 0.591701743836 0.992042602681
0.126725726459  - val loss
0.126061707606  - final_loss
Inside Plateau 1



46  iteration
3.90625e-05  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
39s - loss: 0.1421 - acc: 0.9486 - val_loss: 0.1270 - val_acc: 0.9548
(180000,) (180000,)
160189 1969
7223 10619

FA FR TA TR 0.0121424783236 0.40483129694 0.59516870306 0.987857521676

VALIDATION DATA
0.954833333333 0.12696574835
(18000,) (18000,)
16200 137
676 987

FA FR TA TR 0.00838587255922 0.406494287432 0.593505712568 0.991614127441
0.12696574835  - val loss
0.126061707606  - final_loss
Inside Plateau 2



46  iteration
3.90625e-05  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
39s - loss: 0.1423 - acc: 0.9487 - val_loss: 0.1269 - val_acc: 0.9551
(180000,) (180000,)
160210 1948
7250 10592

FA FR TA TR 0.0120129749997 0.406344580204 0.593655419796 0.987987025

VALIDATION DATA
0.955055555556 0.12690548816
(18000,) (18000,)
16204 133
676 987

FA FR TA TR 0.00814102956479 0.406494287432 0.593505712568 0.991858970435
0.12690548816  - val loss
0.126061707606  - final_loss
Inside Plateau 3



46  iteration
3.90625e-05  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
39s - loss: 0.1419 - acc: 0.9489 - val_loss: 0.1272 - val_acc: 0.9551
(180000,) (180000,)
160147 2011
7163 10679

FA FR TA TR 0.0124014849714 0.401468445242 0.598531554758 0.987598515029

VALIDATION DATA
0.955055555556 0.127218420578
(18000,) (18000,)
16199 138
671 992

FA FR TA TR 0.00844708330783 0.40348767288 0.59651232712 0.991552916692
0.127218420578  - val loss
0.126061707606  - final_loss
Reducing the learning rate by half
   32/18000 [..............................] - ETA: 1s  736/18000 [>.............................] - ETA: 1s 1472/18000 [=>............................] - ETA: 1s 2240/18000 [==>...........................] - ETA: 1s 2976/18000 [===>..........................] - ETA: 1s 3776/18000 [=====>........................] - ETA: 0s 4576/18000 [======>.......................] - ETA: 0s 5376/18000 [=======>......................] - ETA: 0s 6176/18000 [=========>....................] - ETA: 0s 7008/18000 [==========>...................] - ETA: 0s 7808/18000 [============>.................] - ETA: 0s 8512/18000 [=============>................] - ETA: 0s 9248/18000 [==============>...............] - ETA: 0s10016/18000 [===============>..............] - ETA: 0s10720/18000 [================>.............] - ETA: 0s11456/18000 [==================>...........] - ETA: 0s12192/18000 [===================>..........] - ETA: 0s12960/18000 [====================>.........] - ETA: 0s13728/18000 [=====================>........] - ETA: 0s14496/18000 [=======================>......] - ETA: 0s15232/18000 [========================>.....] - ETA: 0s16000/18000 [=========================>....] - ETA: 0s16736/18000 [==========================>...] - ETA: 0s17504/18000 [============================>.] - ETA: 0s
ROC AREA:  0.967668191807
(18000,) (18000,)

train images/masks shape:
(18, 1, 565, 565)
train images range (min-max): 0.0 - 1.0
train masks are within 0-1


train images/masks shape:
(18, 1, 565, 565)
train images range (min-max): 0.0 - 1.0
train masks are within 0-1


train images/masks shape:
(18, 1, 565, 565)
train images range (min-max): 0.0 - 1.0
train masks are within 0-1

N_patches: plase enter a multiple of  18

train images/masks shape:
(18, 1, 565, 565)
train images range (min-max): 0.0 - 1.0
train masks are within 0-1


train images/masks shape:
(18, 1, 565, 565)
train images range (min-max): 0.0 - 1.0
train masks are within 0-1

positive patches per full image: 5000
negative patches per full image: 5000

train PATCHES images/masks shape:
(180000, 1, 27, 27)
train PATCHES images range (min-max): 0.0 - 1.0

train images/masks shape:
(18, 1, 565, 565)
train images range (min-max): 0.0 - 1.0
train masks are within 0-1

positive patches per full image: 5000
negative patches per full image: 5000

train PATCHES images/masks shape:
(180000, 1, 27, 27)
train PATCHES images range (min-max): 0.0 - 1.0

train images/masks shape:
(2, 1, 565, 565)
train images range (min-max): 0.0 - 1.0
train masks are within 0-1

patches per full image: 9000

train PATCHES images/masks shape:
(18000, 1, 27, 27)
train PATCHES images range (min-max): 0.0 - 1.0
Check: final output of the network:
(None, 2)
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 1, 27, 27)         0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 32, 27, 27)        320       
_________________________________________________________________
dropout_1 (Dropout)          (None, 32, 27, 27)        0         
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 32, 13, 13)        0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 5408)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 2)                 10818     
_________________________________________________________________
activation_1 (Activation)    (None, 2)                 0         
=================================================================
Total params: 11,138
Trainable params: 11,138
Non-trainable params: 0
_________________________________________________________________



1  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from inf to 0.42575, saving model to ./log/cnn/log-weights-0.42575.h5
Epoch 00000: val_loss improved from inf to 0.42575, saving model to ./log/cnn/log_best_weights.h5
37s - loss: 0.5621 - acc: 0.6819 - val_loss: 0.4257 - val_acc: 0.9108
(180000,) (180000,)
83101 6899
31136 58864

FA FR TA TR 0.0766555555556 0.345955555556 0.654044444444 0.923344444444

VALIDATION DATA
0.910777777778 0.425748399761
(18000,) (18000,)
15407 992
614 987

FA FR TA TR 0.0604914933837 0.383510306059 0.616489693941 0.939508506616
0.425748399761  - val loss
999999  - final_loss
Validation Loss decreased. Great work



2  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
36s - loss: 0.4626 - acc: 0.7910 - val_loss: 0.4280 - val_acc: 0.8708
(180000,) (180000,)
77425 12575
18927 71073

FA FR TA TR 0.139722222222 0.2103 0.7897 0.860277777778

VALIDATION DATA
0.870777777778 0.427970512576
(18000,) (18000,)
14413 1986
340 1261

FA FR TA TR 0.121104945424 0.212367270456 0.787632729544 0.878895054576
0.427970512576  - val loss
0.425748399761  - final_loss
Inside Plateau 1



2  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from 0.42575 to 0.29371, saving model to ./log/cnn/log-weights-0.29371.h5
Epoch 00000: val_loss improved from 0.42575 to 0.29371, saving model to ./log/cnn/log_best_weights.h5
36s - loss: 0.4267 - acc: 0.8123 - val_loss: 0.2937 - val_acc: 0.9349
(180000,) (180000,)
85803 4197
33378 56622

FA FR TA TR 0.0466333333333 0.370866666667 0.629133333333 0.953366666667

VALIDATION DATA
0.934944444444 0.29370708206
(18000,) (18000,)
15919 480
691 910

FA FR TA TR 0.0292700774437 0.431605246721 0.568394753279 0.970729922556
0.29370708206  - val loss
0.425748399761  - final_loss
Validation Loss decreased. Great work



3  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
36s - loss: 0.4042 - acc: 0.8258 - val_loss: 0.3837 - val_acc: 0.8857
(180000,) (180000,)
80390 9610
17759 72241

FA FR TA TR 0.106777777778 0.197322222222 0.802677777778 0.893222222222

VALIDATION DATA
0.885722222222 0.383712033272
(18000,) (18000,)
14608 1791
266 1335

FA FR TA TR 0.109213976462 0.166146158651 0.833853841349 0.890786023538
0.383712033272  - val loss
0.29370708206  - final_loss
Inside Plateau 1



3  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
36s - loss: 0.3834 - acc: 0.8357 - val_loss: 0.5202 - val_acc: 0.7228
(180000,) (180000,)
64198 25802
5301 84699

FA FR TA TR 0.286688888889 0.0589 0.9411 0.713311111111

VALIDATION DATA
0.722833333333 0.520234249274
(18000,) (18000,)
11477 4922
67 1534

FA FR TA TR 0.300140252454 0.0418488444722 0.958151155528 0.699859747546
0.520234249274  - val loss
0.29370708206  - final_loss
Inside Plateau 2



3  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from 0.29371 to 0.23137, saving model to ./log/cnn/log-weights-0.23137.h5
Epoch 00000: val_loss improved from 0.29371 to 0.23137, saving model to ./log/cnn/log_best_weights.h5
36s - loss: 0.3673 - acc: 0.8450 - val_loss: 0.2314 - val_acc: 0.9444
(180000,) (180000,)
87260 2740
33063 56937

FA FR TA TR 0.0304444444444 0.367366666667 0.632633333333 0.969555555556

VALIDATION DATA
0.944444444444 0.231370019688
(18000,) (18000,)
16068 331
669 932

FA FR TA TR 0.0201841575706 0.417863835103 0.582136164897 0.979815842429
0.231370019688  - val loss
0.29370708206  - final_loss
Validation Loss decreased. Great work



4  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
36s - loss: 0.3510 - acc: 0.8534 - val_loss: 0.3197 - val_acc: 0.9227
(180000,) (180000,)
82795 7205
17191 72809

FA FR TA TR 0.0800555555556 0.191011111111 0.808988888889 0.919944444444

VALIDATION DATA
0.922666666667 0.31974531081
(18000,) (18000,)
15314 1085
307 1294

FA FR TA TR 0.0661625708885 0.191755153029 0.808244846971 0.933837429112
0.31974531081  - val loss
0.231370019688  - final_loss
Inside Plateau 1



4  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
36s - loss: 0.3412 - acc: 0.8592 - val_loss: 0.3201 - val_acc: 0.9165
(180000,) (180000,)
82961 7039
15904 74096

FA FR TA TR 0.0782111111111 0.176711111111 0.823288888889 0.921788888889

VALIDATION DATA
0.9165 0.320102701214
(18000,) (18000,)
15129 1270
233 1368

FA FR TA TR 0.0774437465699 0.145534041224 0.854465958776 0.92255625343
0.320102701214  - val loss
0.231370019688  - final_loss
Inside Plateau 2



4  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
36s - loss: 0.3310 - acc: 0.8645 - val_loss: 0.6502 - val_acc: 0.5713
(180000,) (180000,)
47295 42705
1619 88381

FA FR TA TR 0.4745 0.0179888888889 0.982011111111 0.5255

VALIDATION DATA
0.571333333333 0.650170893298
(18000,) (18000,)
8702 7697
19 1582

FA FR TA TR 0.469357887676 0.0118675827608 0.988132417239 0.530642112324
0.650170893298  - val loss
0.231370019688  - final_loss
Inside Plateau 3



4  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from 0.23137 to 0.21086, saving model to ./log/cnn/log-weights-0.21086.h5
Epoch 00000: val_loss improved from 0.23137 to 0.21086, saving model to ./log/cnn/log_best_weights.h5
36s - loss: 0.3235 - acc: 0.8689 - val_loss: 0.2109 - val_acc: 0.9441
(180000,) (180000,)
86619 3381
24877 65123

FA FR TA TR 0.0375666666667 0.276411111111 0.723588888889 0.962433333333

VALIDATION DATA
0.944111111111 0.210860790531
(18000,) (18000,)
15831 568
438 1163

FA FR TA TR 0.0346362583084 0.273579013117 0.726420986883 0.965363741692
0.210860790531  - val loss
0.231370019688  - final_loss
Validation Loss decreased. Great work



5  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
36s - loss: 0.3183 - acc: 0.8717 - val_loss: 0.2473 - val_acc: 0.9376
(180000,) (180000,)
84565 5435
17393 72607

FA FR TA TR 0.0603888888889 0.193255555556 0.806744444444 0.939611111111

VALIDATION DATA
0.937555555556 0.2473444576
(18000,) (18000,)
15579 820
304 1297

FA FR TA TR 0.0500030489664 0.189881324172 0.810118675828 0.949996951034
0.2473444576  - val loss
0.210860790531  - final_loss
Inside Plateau 1



5  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
36s - loss: 0.3134 - acc: 0.8738 - val_loss: 0.3849 - val_acc: 0.8724
(180000,) (180000,)
77112 12888
7461 82539

FA FR TA TR 0.1432 0.0829 0.9171 0.8568

VALIDATION DATA
0.872388888889 0.384943030967
(18000,) (18000,)
14200 2199
98 1503

FA FR TA TR 0.134093542289 0.0612117426608 0.938788257339 0.865906457711
0.384943030967  - val loss
0.210860790531  - final_loss
Inside Plateau 2



5  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss improved from 0.21086 to 0.20807, saving model to ./log/cnn/log-weights-0.20807.h5
Epoch 00000: val_loss improved from 0.21086 to 0.20807, saving model to ./log/cnn/log_best_weights.h5
36s - loss: 0.3098 - acc: 0.8756 - val_loss: 0.2081 - val_acc: 0.9450
(180000,) (180000,)
86506 3494
23050 66950

FA FR TA TR 0.0388222222222 0.256111111111 0.743888888889 0.961177777778

VALIDATION DATA
0.945 0.208066462292
(18000,) (18000,)
15777 622
368 1233

FA FR TA TR 0.0379291420209 0.229856339788 0.770143660212 0.962070857979
0.208066462292  - val loss
0.210860790531  - final_loss
Validation Loss decreased. Great work



6  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
36s - loss: 0.3072 - acc: 0.8780 - val_loss: 0.2376 - val_acc: 0.9394
(180000,) (180000,)
84744 5256
16823 73177

FA FR TA TR 0.0584 0.186922222222 0.813077777778 0.9416

VALIDATION DATA
0.939388888889 0.237571659115
(18000,) (18000,)
15596 803
288 1313

FA FR TA TR 0.0489664003903 0.179887570269 0.820112429731 0.95103359961
0.237571659115  - val loss
0.208066462292  - final_loss
Inside Plateau 1



6  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
36s - loss: 0.3047 - acc: 0.8794 - val_loss: 0.3234 - val_acc: 0.9103
(180000,) (180000,)
80459 9541
9749 80251

FA FR TA TR 0.106011111111 0.108322222222 0.891677777778 0.893988888889

VALIDATION DATA
0.910277777778 0.323399234295
(18000,) (18000,)
14922 1477
138 1463

FA FR TA TR 0.0900664674675 0.0861961274204 0.91380387258 0.909933532532
0.323399234295  - val loss
0.208066462292  - final_loss
Inside Plateau 2



6  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
37s - loss: 0.3028 - acc: 0.8803 - val_loss: 0.2134 - val_acc: 0.9436
(180000,) (180000,)
85859 4141
20242 69758

FA FR TA TR 0.0460111111111 0.224911111111 0.775088888889 0.953988888889

VALIDATION DATA
0.943611111111 0.213366339154
(18000,) (18000,)
15729 670
345 1256

FA FR TA TR 0.0408561497652 0.215490318551 0.784509681449 0.959143850235
0.213366339154  - val loss
0.208066462292  - final_loss
Inside Plateau 3



6  iteration
0.01  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
36s - loss: 0.3008 - acc: 0.8820 - val_loss: 0.2385 - val_acc: 0.9373
(180000,) (180000,)
84726 5274
15690 74310

FA FR TA TR 0.0586 0.174333333333 0.825666666667 0.9414

VALIDATION DATA
0.937277777778 0.238495292624
(18000,) (18000,)
15512 887
242 1359

FA FR TA TR 0.0540886639429 0.151155527795 0.848844472205 0.945911336057
0.238495292624  - val loss
0.208066462292  - final_loss
Reducing the learning rate by half



6  iteration
0.005  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
37s - loss: 0.2909 - acc: 0.8878 - val_loss: 0.2876 - val_acc: 0.9234
(180000,) (180000,)
82820 7180
12095 77905

FA FR TA TR 0.0797777777778 0.134388888889 0.865611111111 0.920222222222

VALIDATION DATA
0.923444444444 0.287614414162
(18000,) (18000,)
15188 1211
167 1434

FA FR TA TR 0.0738459662175 0.104309806371 0.895690193629 0.926154033783
0.287614414162  - val loss
0.208066462292  - final_loss
Inside Plateau 1



6  iteration
0.005  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
37s - loss: 0.2901 - acc: 0.8879 - val_loss: 0.3140 - val_acc: 0.9118
(180000,) (180000,)
81597 8403
10539 79461

FA FR TA TR 0.0933666666667 0.1171 0.8829 0.906633333333

VALIDATION DATA
0.911777777778 0.31395081634
(18000,) (18000,)
14946 1453
135 1466

FA FR TA TR 0.0886029635953 0.0843222985634 0.915677701437 0.911397036405
0.31395081634  - val loss
0.208066462292  - final_loss
Inside Plateau 2



6  iteration
0.005  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
37s - loss: 0.2893 - acc: 0.8883 - val_loss: 0.2207 - val_acc: 0.9407
(180000,) (180000,)
85120 4880
17483 72517

FA FR TA TR 0.0542222222222 0.194255555556 0.805744444444 0.945777777778

VALIDATION DATA
0.940722222222 0.220691782819
(18000,) (18000,)
15626 773
294 1307

FA FR TA TR 0.04713702055 0.183635227983 0.816364772017 0.95286297945
0.220691782819  - val loss
0.208066462292  - final_loss
Inside Plateau 3



6  iteration
0.005  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
36s - loss: 0.2885 - acc: 0.8883 - val_loss: 0.2607 - val_acc: 0.9310
(180000,) (180000,)
83737 6263
13535 76465

FA FR TA TR 0.0695888888889 0.150388888889 0.849611111111 0.930411111111

VALIDATION DATA
0.931 0.26072749771
(18000,) (18000,)
15359 1040
202 1399

FA FR TA TR 0.0634185011281 0.126171143036 0.873828856964 0.936581498872
0.26072749771  - val loss
0.208066462292  - final_loss
Reducing the learning rate by half



6  iteration
0.0025  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
37s - loss: 0.2844 - acc: 0.8908 - val_loss: 0.3619 - val_acc: 0.8867
(180000,) (180000,)
78137 11863
7313 82687

FA FR TA TR 0.131811111111 0.0812555555556 0.918744444444 0.868188888889

VALIDATION DATA
0.886666666667 0.36191872655
(18000,) (18000,)
14445 1954
86 1515

FA FR TA TR 0.119153606927 0.053716427233 0.946283572767 0.880846393073
0.36191872655  - val loss
0.208066462292  - final_loss
Inside Plateau 1



6  iteration
0.0025  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
37s - loss: 0.2845 - acc: 0.8908 - val_loss: 0.2828 - val_acc: 0.9233
(180000,) (180000,)
82642 7358
11626 78374

FA FR TA TR 0.0817555555556 0.129177777778 0.870822222222 0.918244444444

VALIDATION DATA
0.923333333333 0.282790214194
(18000,) (18000,)
15184 1215
165 1436

FA FR TA TR 0.0740898835295 0.103060587133 0.896939412867 0.925910116471
0.282790214194  - val loss
0.208066462292  - final_loss
Inside Plateau 2



6  iteration
0.0025  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
37s - loss: 0.2839 - acc: 0.8909 - val_loss: 0.3260 - val_acc: 0.9054
(180000,) (180000,)
79686 10314
8487 81513

FA FR TA TR 0.1146 0.0943 0.9057 0.8854

VALIDATION DATA
0.905388888889 0.32602425366
(18000,) (18000,)
14808 1591
112 1489

FA FR TA TR 0.0970181108604 0.0699562773267 0.930043722673 0.90298188914
0.32602425366  - val loss
0.208066462292  - final_loss
Inside Plateau 3



6  iteration
0.0025  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
37s - loss: 0.2836 - acc: 0.8910 - val_loss: 0.2771 - val_acc: 0.9282
(180000,) (180000,)
82761 7239
11710 78290

FA FR TA TR 0.0804333333333 0.130111111111 0.869888888889 0.919566666667

VALIDATION DATA
0.928166666667 0.277070488718
(18000,) (18000,)
15276 1123
170 1431

FA FR TA TR 0.0684797853528 0.106183635228 0.893816364772 0.931520214647
0.277070488718  - val loss
0.208066462292  - final_loss
Reducing the learning rate by half



6  iteration
0.00125  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
37s - loss: 0.2822 - acc: 0.8924 - val_loss: 0.2826 - val_acc: 0.9234
(180000,) (180000,)
82485 7515
11389 78611

FA FR TA TR 0.0835 0.126544444444 0.873455555556 0.9165

VALIDATION DATA
0.923444444444 0.282612229175
(18000,) (18000,)
15179 1220
158 1443

FA FR TA TR 0.0743947801695 0.0986883198001 0.9013116802 0.92560521983
0.282612229175  - val loss
0.208066462292  - final_loss
Inside Plateau 1



6  iteration
0.00125  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
37s - loss: 0.2822 - acc: 0.8919 - val_loss: 0.2887 - val_acc: 0.9222
(180000,) (180000,)
82298 7702
11144 78856

FA FR TA TR 0.0855777777778 0.123822222222 0.876177777778 0.914422222222

VALIDATION DATA
0.922222222222 0.288682566855
(18000,) (18000,)
15149 1250
150 1451

FA FR TA TR 0.0762241600098 0.0936914428482 0.906308557152 0.92377583999
0.288682566855  - val loss
0.208066462292  - final_loss
Inside Plateau 2



6  iteration
0.00125  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
37s - loss: 0.2816 - acc: 0.8921 - val_loss: 0.2714 - val_acc: 0.9270
(180000,) (180000,)
82973 7027
12084 77916

FA FR TA TR 0.0780777777778 0.134266666667 0.865733333333 0.921922222222

VALIDATION DATA
0.927 0.271412249764
(18000,) (18000,)
15259 1140
174 1427

FA FR TA TR 0.0695164339289 0.108682073704 0.891317926296 0.930483566071
0.271412249764  - val loss
0.208066462292  - final_loss
Inside Plateau 3



6  iteration
0.00125  learning rate

TRAIN DATA
Train on 180000 samples, validate on 18000 samples
Epoch 1/1
Epoch 00000: val_loss did not improve
Epoch 00000: val_loss did not improve
37s - loss: 0.2816 - acc: 0.8920 - val_loss: 0.3119 - val_acc: 0.9130
(180000,) (180000,)
80742 9258
9371 80629

FA FR TA TR 0.102866666667 0.104122222222 0.895877777778 0.897133333333

VALIDATION DATA
0.913 0.311855073426
(18000,) (18000,)
14954 1445
121 1480

FA FR TA TR 0.0881151289713 0.0755777638976 0.924422236102 0.911884871029
0.311855073426  - val loss
0.208066462292  - final_loss
Reducing the learning rate by half
   32/18000 [..............................] - ETA: 1s  896/18000 [>.............................] - ETA: 1s 1792/18000 [=>............................] - ETA: 0s 2688/18000 [===>..........................] - ETA: 0s 3584/18000 [====>.........................] - ETA: 0s 4448/18000 [======>.......................] - ETA: 0s 5344/18000 [=======>......................] - ETA: 0s 6208/18000 [=========>....................] - ETA: 0s 7072/18000 [==========>...................] - ETA: 0s 7968/18000 [============>.................] - ETA: 0s 8800/18000 [=============>................] - ETA: 0s 9664/18000 [===============>..............] - ETA: 0s10528/18000 [================>.............] - ETA: 0s11424/18000 [==================>...........] - ETA: 0s12288/18000 [===================>..........] - ETA: 0s13152/18000 [====================>.........] - ETA: 0s14016/18000 [======================>.......] - ETA: 0s14848/18000 [=======================>......] - ETA: 0s15680/18000 [=========================>....] - ETA: 0s16480/18000 [==========================>...] - ETA: 0s17248/18000 [===========================>..] - ETA: 0s
ROC AREA:  0.961776207085
(18000,) (18000,)
